{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.func import vmap, jacrev, functional_call\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boston Dataset :\n",
    "\n",
    "# Prédiction du prix médian de maisons dans différentes banlieues de Boston, en fonction de plusieurs variables \n",
    "# socio-économiques, géographiques et environnementales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Boston.csv').drop('Unnamed: 0', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonDataset(Dataset):\n",
    "    def __init__(self, path=\"Boston.csv\"):\n",
    "        data = pd.read_csv(path).drop('Unnamed: 0', axis=1)\n",
    "        self.features = data.drop('medv', axis=1)\n",
    "        self.targets = data[\"medv\"]\n",
    "        self.features = StandardScaler().fit_transform(self.features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.Tensor(self.features[index]), torch.Tensor([self.targets[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 13])\n"
     ]
    }
   ],
   "source": [
    "dataset = BostonDataset()\n",
    "\n",
    "x_ntk = torch.stack([dataset[i][0] for i in range(100)])\n",
    "print(x_ntk.shape)\n",
    "\n",
    "# NTK shape : 100x100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(13, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(hidden_dim=10)\n",
    "parameters = {k:v.detach() for k, v in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On a besoin d'une fonction qui prend en entrée les paramètres du réseau, et une observation \n",
    "# et qui retourne la prédiction du réseau, pour calculer le gradient par rapport aux paramètres.\n",
    "\n",
    "def fnet_single(params, x):\n",
    "    return functional_call(model, params, (x.unsqueeze(0), )).squeeze(0)\n",
    "\n",
    "fnet_single(parameters, x_ntk[0]) == model(x_ntk[0].unsqueeze(0)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On va s'intéresser aux jacobians :\n",
    "\n",
    "def f(x):\n",
    "    return 2*torch.exp(x) + 6*x\n",
    "\n",
    "def derivate(x):\n",
    "    return 2*torch.exp(x) + 6\n",
    "\n",
    "x = torch.tensor(2.0).unsqueeze(0)\n",
    "jacrev(f, argnums=0)(x)==derivate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian de fnet_single : donc du modèle par rapport aux paramètres :\n",
    "\n",
    "jac = jacrev(fnet_single, argnums=0)(parameters, x_ntk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap : permet de vectoriser les calculs :\n",
    "\n",
    "jac = vmap(jacrev(fnet_single), (None, 0))(parameters, x_ntk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac = jac.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac = [j.flatten(1) for j in jac]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "result = torch.stack([torch.einsum('Na, Mb -> NM', j, j) for j in jac])\n",
    "print(result.shape) # dimension 4 parce que : weight0, bias0, weight2, bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "result = result.sum(0)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ntk(fnet_single, params, x_ntk):\n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, x_ntk)\n",
    "    jac = jac.values()\n",
    "    jac = [j.flatten(1) for j in jac]\n",
    "\n",
    "    result = torch.stack([torch.einsum(\"Na, Mb -> NM\", j, j) for j in jac])\n",
    "    result = result.sum(0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "ntk = get_ntk(fnet_single, parameters, x_ntk)\n",
    "print(ntk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_norm(ntk_t, ntk_init):\n",
    "    # Afin de voir l'évolution du NTK au cours de l'entraînement\n",
    "    rel_norm = torch.linalg.norm(ntk_t - ntk_init)**2 / torch.linalg.norm(ntk_init)**2\n",
    "    return rel_norm.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    50:[[0] for _ in range(10)],\n",
    "    100:[[0] for _ in range(10)],\n",
    "    500:[[0] for _ in range(10)],\n",
    "    1000:[[0] for _ in range(10)],\n",
    "    5000:[[0] for _ in range(10)],\n",
    "    10000:[[0] for _ in range(10)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Working on model 50 , iter 0 ***\n",
      "for epoch 0 ; training loss : 294635.1727657318\n",
      "for epoch 1 ; training loss : 293505.7222805023\n",
      "for epoch 2 ; training loss : 292368.780708313\n",
      "for epoch 3 ; training loss : 291219.75239372253\n",
      "for epoch 4 ; training loss : 290058.0082397461\n",
      "for epoch 5 ; training loss : 288884.39723587036\n",
      "for epoch 6 ; training loss : 287697.97096061707\n",
      "for epoch 7 ; training loss : 286496.8877429962\n",
      "for epoch 8 ; training loss : 285280.2931537628\n",
      "for epoch 9 ; training loss : 284047.646774292\n",
      "for epoch 10 ; training loss : 282795.24979400635\n",
      "for epoch 11 ; training loss : 281524.4274339676\n",
      "for epoch 12 ; training loss : 280233.2247495651\n",
      "for epoch 13 ; training loss : 278919.7860651016\n",
      "for epoch 14 ; training loss : 277583.03108406067\n",
      "for epoch 15 ; training loss : 276222.1737527847\n",
      "for epoch 16 ; training loss : 274834.10724544525\n",
      "for epoch 17 ; training loss : 273416.7699403763\n",
      "for epoch 18 ; training loss : 271969.01593112946\n",
      "for epoch 19 ; training loss : 270491.20239162445\n",
      "for epoch 20 ; training loss : 268983.2634305954\n",
      "for epoch 21 ; training loss : 267443.40149593353\n",
      "for epoch 22 ; training loss : 265870.59047937393\n",
      "for epoch 23 ; training loss : 264264.75480413437\n",
      "for epoch 24 ; training loss : 262627.0390782356\n",
      "for epoch 25 ; training loss : 260957.2927827835\n",
      "for epoch 26 ; training loss : 259254.87775230408\n",
      "for epoch 27 ; training loss : 257518.23777532578\n",
      "for epoch 28 ; training loss : 255747.4652504921\n",
      "for epoch 29 ; training loss : 253943.2442831993\n",
      "for epoch 30 ; training loss : 252105.55920910835\n",
      "for epoch 31 ; training loss : 250233.96779370308\n",
      "for epoch 32 ; training loss : 248329.15266680717\n",
      "for epoch 33 ; training loss : 246390.92529511452\n",
      "for epoch 34 ; training loss : 244419.43890857697\n",
      "for epoch 35 ; training loss : 242417.40233373642\n",
      "for epoch 36 ; training loss : 240384.5970146656\n",
      "for epoch 37 ; training loss : 238321.23026394844\n",
      "for epoch 38 ; training loss : 236226.4233351946\n",
      "for epoch 39 ; training loss : 234099.58224469423\n",
      "for epoch 40 ; training loss : 231939.79158517718\n",
      "for epoch 41 ; training loss : 229746.70535317063\n",
      "for epoch 42 ; training loss : 227521.83071626723\n",
      "for epoch 43 ; training loss : 225267.6199370958\n",
      "for epoch 44 ; training loss : 222984.19687798806\n",
      "for epoch 45 ; training loss : 220670.7329186094\n",
      "for epoch 46 ; training loss : 218327.54073939845\n",
      "for epoch 47 ; training loss : 215956.14238818735\n",
      "for epoch 48 ; training loss : 213557.39627631754\n",
      "for epoch 49 ; training loss : 211131.46214509197\n",
      "*** Working on model 50 , iter 1 ***\n",
      "for epoch 0 ; training loss : 296138.8699989319\n",
      "for epoch 1 ; training loss : 294803.871011734\n",
      "for epoch 2 ; training loss : 293454.34529685974\n",
      "for epoch 3 ; training loss : 292089.4649925232\n",
      "for epoch 4 ; training loss : 290710.33162117004\n",
      "for epoch 5 ; training loss : 289316.427690506\n",
      "for epoch 6 ; training loss : 287909.15927505493\n",
      "for epoch 7 ; training loss : 286486.4542121887\n",
      "for epoch 8 ; training loss : 285047.00175476074\n",
      "for epoch 9 ; training loss : 283589.8339767456\n",
      "for epoch 10 ; training loss : 282114.91644096375\n",
      "for epoch 11 ; training loss : 280621.4417476654\n",
      "for epoch 12 ; training loss : 279109.4624824524\n",
      "for epoch 13 ; training loss : 277575.3295688629\n",
      "for epoch 14 ; training loss : 276016.8904361725\n",
      "for epoch 15 ; training loss : 274433.480304718\n",
      "for epoch 16 ; training loss : 272825.39569854736\n",
      "for epoch 17 ; training loss : 271191.9191646576\n",
      "for epoch 18 ; training loss : 269532.14437294006\n",
      "for epoch 19 ; training loss : 267844.92667770386\n",
      "for epoch 20 ; training loss : 266129.3406820297\n",
      "for epoch 21 ; training loss : 264385.48221874237\n",
      "for epoch 22 ; training loss : 262612.0842075348\n",
      "for epoch 23 ; training loss : 260806.9042339325\n",
      "for epoch 24 ; training loss : 258970.33984851837\n",
      "for epoch 25 ; training loss : 257099.82367038727\n",
      "for epoch 26 ; training loss : 255196.00222301483\n",
      "for epoch 27 ; training loss : 253260.27392482758\n",
      "for epoch 28 ; training loss : 251291.22075939178\n",
      "for epoch 29 ; training loss : 249287.88751602173\n",
      "for epoch 30 ; training loss : 247250.82997512817\n",
      "for epoch 31 ; training loss : 245177.86216926575\n",
      "for epoch 32 ; training loss : 243070.00031995773\n",
      "for epoch 33 ; training loss : 240925.85210466385\n",
      "for epoch 34 ; training loss : 238745.0618376732\n",
      "for epoch 35 ; training loss : 236528.6163520813\n",
      "for epoch 36 ; training loss : 234277.94439458847\n",
      "for epoch 37 ; training loss : 231993.4573149681\n",
      "for epoch 38 ; training loss : 229674.54074954987\n",
      "for epoch 39 ; training loss : 227321.55577373505\n",
      "for epoch 40 ; training loss : 224934.9483652115\n",
      "for epoch 41 ; training loss : 222515.35122394562\n",
      "for epoch 42 ; training loss : 220064.32979536057\n",
      "for epoch 43 ; training loss : 217581.3015241623\n",
      "for epoch 44 ; training loss : 215067.97169399261\n",
      "for epoch 45 ; training loss : 212525.24063944817\n",
      "for epoch 46 ; training loss : 209954.16862726212\n",
      "for epoch 47 ; training loss : 207355.52425384521\n",
      "for epoch 48 ; training loss : 204730.23660707474\n",
      "for epoch 49 ; training loss : 202079.94631421566\n",
      "*** Working on model 50 , iter 2 ***\n",
      "for epoch 0 ; training loss : 297995.6375427246\n",
      "for epoch 1 ; training loss : 296793.9771785736\n",
      "for epoch 2 ; training loss : 295598.79796409607\n",
      "for epoch 3 ; training loss : 294402.0277271271\n",
      "for epoch 4 ; training loss : 293203.184677124\n",
      "for epoch 5 ; training loss : 292001.54441070557\n",
      "for epoch 6 ; training loss : 290794.7280139923\n",
      "for epoch 7 ; training loss : 289581.00467300415\n",
      "for epoch 8 ; training loss : 288358.0088710785\n",
      "for epoch 9 ; training loss : 287121.5984096527\n",
      "for epoch 10 ; training loss : 285869.37258529663\n",
      "for epoch 11 ; training loss : 284599.8256702423\n",
      "for epoch 12 ; training loss : 283311.57284736633\n",
      "for epoch 13 ; training loss : 282004.5958404541\n",
      "for epoch 14 ; training loss : 280679.80525302887\n",
      "for epoch 15 ; training loss : 279335.9109067917\n",
      "for epoch 16 ; training loss : 277972.4321527481\n",
      "for epoch 17 ; training loss : 276588.72817993164\n",
      "for epoch 18 ; training loss : 275185.8402929306\n",
      "for epoch 19 ; training loss : 273762.50465393066\n",
      "for epoch 20 ; training loss : 272317.2489490509\n",
      "for epoch 21 ; training loss : 270849.64214992523\n",
      "for epoch 22 ; training loss : 269359.3793525696\n",
      "for epoch 23 ; training loss : 267846.12407302856\n",
      "for epoch 24 ; training loss : 266307.8029651642\n",
      "for epoch 25 ; training loss : 264741.5233621597\n",
      "for epoch 26 ; training loss : 263146.84410619736\n",
      "for epoch 27 ; training loss : 261523.3975725174\n",
      "for epoch 28 ; training loss : 259870.62793970108\n",
      "for epoch 29 ; training loss : 258187.75441980362\n",
      "for epoch 30 ; training loss : 256474.78559589386\n",
      "for epoch 31 ; training loss : 254730.8949508667\n",
      "for epoch 32 ; training loss : 252955.42335796356\n",
      "for epoch 33 ; training loss : 251150.25810050964\n",
      "for epoch 34 ; training loss : 249314.97299289703\n",
      "for epoch 35 ; training loss : 247447.4416692257\n",
      "for epoch 36 ; training loss : 245546.90221977234\n",
      "for epoch 37 ; training loss : 243616.85701656342\n",
      "for epoch 38 ; training loss : 241657.32623434067\n",
      "for epoch 39 ; training loss : 239668.4678608179\n",
      "for epoch 40 ; training loss : 237649.04467248917\n",
      "for epoch 41 ; training loss : 235601.57012224197\n",
      "for epoch 42 ; training loss : 233526.72815287113\n",
      "for epoch 43 ; training loss : 231422.47375375032\n",
      "for epoch 44 ; training loss : 229289.86740344763\n",
      "for epoch 45 ; training loss : 227130.8136653006\n",
      "for epoch 46 ; training loss : 224946.00377047062\n",
      "for epoch 47 ; training loss : 222735.7729140073\n",
      "for epoch 48 ; training loss : 220500.47473894805\n",
      "for epoch 49 ; training loss : 218239.63159780204\n",
      "*** Working on model 50 , iter 3 ***\n",
      "for epoch 0 ; training loss : 298605.3681640625\n",
      "for epoch 1 ; training loss : 297449.035446167\n",
      "for epoch 2 ; training loss : 296280.9743862152\n",
      "for epoch 3 ; training loss : 295099.97945022583\n",
      "for epoch 4 ; training loss : 293905.7661743164\n",
      "for epoch 5 ; training loss : 292700.408826828\n",
      "for epoch 6 ; training loss : 291482.6035556793\n",
      "for epoch 7 ; training loss : 290250.85172367096\n",
      "for epoch 8 ; training loss : 289003.81812763214\n",
      "for epoch 9 ; training loss : 287739.0866765976\n",
      "for epoch 10 ; training loss : 286456.89783859253\n",
      "for epoch 11 ; training loss : 285154.5947780609\n",
      "for epoch 12 ; training loss : 283831.5480709076\n",
      "for epoch 13 ; training loss : 282486.34515476227\n",
      "for epoch 14 ; training loss : 281116.87368392944\n",
      "for epoch 15 ; training loss : 279723.4243812561\n",
      "for epoch 16 ; training loss : 278305.11282253265\n",
      "for epoch 17 ; training loss : 276860.1734070778\n",
      "for epoch 18 ; training loss : 275384.91296339035\n",
      "for epoch 19 ; training loss : 273875.58743572235\n",
      "for epoch 20 ; training loss : 272332.6908888817\n",
      "for epoch 21 ; training loss : 270757.5124912262\n",
      "for epoch 22 ; training loss : 269149.68073391914\n",
      "for epoch 23 ; training loss : 267508.3665187359\n",
      "for epoch 24 ; training loss : 265833.1191031933\n",
      "for epoch 25 ; training loss : 264124.6033909321\n",
      "for epoch 26 ; training loss : 262382.79569864273\n",
      "for epoch 27 ; training loss : 260605.8407894373\n",
      "for epoch 28 ; training loss : 258793.52924978733\n",
      "for epoch 29 ; training loss : 256944.65700554848\n",
      "for epoch 30 ; training loss : 255057.59420263767\n",
      "for epoch 31 ; training loss : 253132.3975018263\n",
      "for epoch 32 ; training loss : 251171.44193553925\n",
      "for epoch 33 ; training loss : 249174.08922958374\n",
      "for epoch 34 ; training loss : 247139.7770728171\n",
      "for epoch 35 ; training loss : 245070.65272966772\n",
      "for epoch 36 ; training loss : 242967.3603173159\n",
      "for epoch 37 ; training loss : 240828.26367581962\n",
      "for epoch 38 ; training loss : 238654.94835815392\n",
      "for epoch 39 ; training loss : 236448.38939236477\n",
      "for epoch 40 ; training loss : 234208.56410101056\n",
      "for epoch 41 ; training loss : 231936.9566515237\n",
      "for epoch 42 ; training loss : 229633.13824519515\n",
      "for epoch 43 ; training loss : 227299.872669518\n",
      "for epoch 44 ; training loss : 224938.54535748437\n",
      "for epoch 45 ; training loss : 222549.40072563477\n",
      "for epoch 46 ; training loss : 220132.5546494972\n",
      "for epoch 47 ; training loss : 217688.15583277307\n",
      "for epoch 48 ; training loss : 215215.1141409874\n",
      "for epoch 49 ; training loss : 212714.60162725672\n",
      "*** Working on model 50 , iter 4 ***\n",
      "for epoch 0 ; training loss : 304798.58622932434\n",
      "for epoch 1 ; training loss : 303533.39387512207\n",
      "for epoch 2 ; training loss : 302275.6241283417\n",
      "for epoch 3 ; training loss : 301019.96600723267\n",
      "for epoch 4 ; training loss : 299767.3217372894\n",
      "for epoch 5 ; training loss : 298513.61274909973\n",
      "for epoch 6 ; training loss : 297258.7998199463\n",
      "for epoch 7 ; training loss : 296002.2579269409\n",
      "for epoch 8 ; training loss : 294742.88212013245\n",
      "for epoch 9 ; training loss : 293480.4628639221\n",
      "for epoch 10 ; training loss : 292212.1214046478\n",
      "for epoch 11 ; training loss : 290936.5286960602\n",
      "for epoch 12 ; training loss : 289652.68396282196\n",
      "for epoch 13 ; training loss : 288357.88356113434\n",
      "for epoch 14 ; training loss : 287054.2317762375\n",
      "for epoch 15 ; training loss : 285739.9870109558\n",
      "for epoch 16 ; training loss : 284413.1559534073\n",
      "for epoch 17 ; training loss : 283074.6058740616\n",
      "for epoch 18 ; training loss : 281722.9412641525\n",
      "for epoch 19 ; training loss : 280355.7298154831\n",
      "for epoch 20 ; training loss : 278970.1297674179\n",
      "for epoch 21 ; training loss : 277562.64092731476\n",
      "for epoch 22 ; training loss : 276129.8446946144\n",
      "for epoch 23 ; training loss : 274670.47248888016\n",
      "for epoch 24 ; training loss : 273182.3506717682\n",
      "for epoch 25 ; training loss : 271665.1891579628\n",
      "for epoch 26 ; training loss : 270119.7691216469\n",
      "for epoch 27 ; training loss : 268546.6732330322\n",
      "for epoch 28 ; training loss : 266943.36742830276\n",
      "for epoch 29 ; training loss : 265307.87930202484\n",
      "for epoch 30 ; training loss : 263638.64376688004\n",
      "for epoch 31 ; training loss : 261936.2923388481\n",
      "for epoch 32 ; training loss : 260200.08893752098\n",
      "for epoch 33 ; training loss : 258428.72117602825\n",
      "for epoch 34 ; training loss : 256620.96459054947\n",
      "for epoch 35 ; training loss : 254778.33901309967\n",
      "for epoch 36 ; training loss : 252901.6350429058\n",
      "for epoch 37 ; training loss : 250989.88550788164\n",
      "for epoch 38 ; training loss : 249042.8597598076\n",
      "for epoch 39 ; training loss : 247061.09630468488\n",
      "for epoch 40 ; training loss : 245044.69087025523\n",
      "for epoch 41 ; training loss : 242990.9659730494\n",
      "for epoch 42 ; training loss : 240900.80744048208\n",
      "for epoch 43 ; training loss : 238776.63975762576\n",
      "for epoch 44 ; training loss : 236616.45063469047\n",
      "for epoch 45 ; training loss : 234423.0679791076\n",
      "for epoch 46 ; training loss : 232195.78096477687\n",
      "for epoch 47 ; training loss : 229935.48956857622\n",
      "for epoch 48 ; training loss : 227641.0136431232\n",
      "for epoch 49 ; training loss : 225313.85614924878\n",
      "*** Working on model 50 , iter 5 ***\n",
      "for epoch 0 ; training loss : 299542.95965766907\n",
      "for epoch 1 ; training loss : 298338.2587451935\n",
      "for epoch 2 ; training loss : 297125.44573020935\n",
      "for epoch 3 ; training loss : 295897.05365371704\n",
      "for epoch 4 ; training loss : 294653.6486225128\n",
      "for epoch 5 ; training loss : 293395.3616390228\n",
      "for epoch 6 ; training loss : 292121.3822259903\n",
      "for epoch 7 ; training loss : 290829.7980928421\n",
      "for epoch 8 ; training loss : 289521.91445446014\n",
      "for epoch 9 ; training loss : 288197.3435268402\n",
      "for epoch 10 ; training loss : 286854.4764881134\n",
      "for epoch 11 ; training loss : 285494.05825805664\n",
      "for epoch 12 ; training loss : 284115.9116201401\n",
      "for epoch 13 ; training loss : 282720.0284290314\n",
      "for epoch 14 ; training loss : 281303.8591785431\n",
      "for epoch 15 ; training loss : 279866.30742263794\n",
      "for epoch 16 ; training loss : 278407.3940973282\n",
      "for epoch 17 ; training loss : 276928.30906009674\n",
      "for epoch 18 ; training loss : 275427.87994384766\n",
      "for epoch 19 ; training loss : 273906.1213622093\n",
      "for epoch 20 ; training loss : 272361.76627635956\n",
      "for epoch 21 ; training loss : 270793.7728600502\n",
      "for epoch 22 ; training loss : 269201.7218942642\n",
      "for epoch 23 ; training loss : 267585.0758342743\n",
      "for epoch 24 ; training loss : 265941.69107699394\n",
      "for epoch 25 ; training loss : 264270.96528196335\n",
      "for epoch 26 ; training loss : 262574.68176198006\n",
      "for epoch 27 ; training loss : 260853.9567503929\n",
      "for epoch 28 ; training loss : 259107.58673608303\n",
      "for epoch 29 ; training loss : 257334.34378516674\n",
      "for epoch 30 ; training loss : 255534.7414456606\n",
      "for epoch 31 ; training loss : 253710.00597321987\n",
      "for epoch 32 ; training loss : 251856.9567528963\n",
      "for epoch 33 ; training loss : 249973.72071063519\n",
      "for epoch 34 ; training loss : 248060.8262449801\n",
      "for epoch 35 ; training loss : 246118.27082496881\n",
      "for epoch 36 ; training loss : 244146.8472916633\n",
      "for epoch 37 ; training loss : 242148.27961617708\n",
      "for epoch 38 ; training loss : 240121.28317103162\n",
      "for epoch 39 ; training loss : 238064.50136507954\n",
      "for epoch 40 ; training loss : 235978.48331344628\n",
      "for epoch 41 ; training loss : 233862.17052867077\n",
      "for epoch 42 ; training loss : 231715.7024602741\n",
      "for epoch 43 ; training loss : 229541.3134328127\n",
      "for epoch 44 ; training loss : 227338.1884457171\n",
      "for epoch 45 ; training loss : 225107.5622793734\n",
      "for epoch 46 ; training loss : 222849.5879593566\n",
      "for epoch 47 ; training loss : 220565.6309636291\n",
      "for epoch 48 ; training loss : 218255.94275627378\n",
      "for epoch 49 ; training loss : 215920.70380923524\n",
      "*** Working on model 50 , iter 6 ***\n",
      "for epoch 0 ; training loss : 297896.26985549927\n",
      "for epoch 1 ; training loss : 296681.55706977844\n",
      "for epoch 2 ; training loss : 295457.5650100708\n",
      "for epoch 3 ; training loss : 294226.61047554016\n",
      "for epoch 4 ; training loss : 292989.35603904724\n",
      "for epoch 5 ; training loss : 291745.3677225113\n",
      "for epoch 6 ; training loss : 290493.4162378311\n",
      "for epoch 7 ; training loss : 289230.94096565247\n",
      "for epoch 8 ; training loss : 287956.8831062317\n",
      "for epoch 9 ; training loss : 286668.8612918854\n",
      "for epoch 10 ; training loss : 285364.91704940796\n",
      "for epoch 11 ; training loss : 284042.59287548065\n",
      "for epoch 12 ; training loss : 282698.7183742523\n",
      "for epoch 13 ; training loss : 281334.7674045563\n",
      "for epoch 14 ; training loss : 279951.0227546692\n",
      "for epoch 15 ; training loss : 278544.703956604\n",
      "for epoch 16 ; training loss : 277113.7000179291\n",
      "for epoch 17 ; training loss : 275657.64731788635\n",
      "for epoch 18 ; training loss : 274175.4389600754\n",
      "for epoch 19 ; training loss : 272666.1843357086\n",
      "for epoch 20 ; training loss : 271128.2475280762\n",
      "for epoch 21 ; training loss : 269561.57497501373\n",
      "for epoch 22 ; training loss : 267965.7335805893\n",
      "for epoch 23 ; training loss : 266341.3369503021\n",
      "for epoch 24 ; training loss : 264686.563706398\n",
      "for epoch 25 ; training loss : 263000.43039274216\n",
      "for epoch 26 ; training loss : 261282.47595977783\n",
      "for epoch 27 ; training loss : 259533.15161418915\n",
      "for epoch 28 ; training loss : 257750.91346549988\n",
      "for epoch 29 ; training loss : 255932.21994495392\n",
      "for epoch 30 ; training loss : 254077.43552970886\n",
      "for epoch 31 ; training loss : 252185.77892637253\n",
      "for epoch 32 ; training loss : 250258.3092737198\n",
      "for epoch 33 ; training loss : 248297.16380882263\n",
      "for epoch 34 ; training loss : 246299.8194270134\n",
      "for epoch 35 ; training loss : 244263.77491903305\n",
      "for epoch 36 ; training loss : 242193.53928422928\n",
      "for epoch 37 ; training loss : 240088.11777305603\n",
      "for epoch 38 ; training loss : 237945.808634758\n",
      "for epoch 39 ; training loss : 235765.19486689568\n",
      "for epoch 40 ; training loss : 233548.17084765434\n",
      "for epoch 41 ; training loss : 231296.12060046196\n",
      "for epoch 42 ; training loss : 229006.47088718414\n",
      "for epoch 43 ; training loss : 226679.4765341282\n",
      "for epoch 44 ; training loss : 224318.7863395214\n",
      "for epoch 45 ; training loss : 221925.52371931076\n",
      "for epoch 46 ; training loss : 219497.60677742958\n",
      "for epoch 47 ; training loss : 217037.3503189087\n",
      "for epoch 48 ; training loss : 214544.65370833874\n",
      "for epoch 49 ; training loss : 212018.95383274555\n",
      "*** Working on model 50 , iter 7 ***\n",
      "for epoch 0 ; training loss : 297835.19919776917\n",
      "for epoch 1 ; training loss : 296519.69260025024\n",
      "for epoch 2 ; training loss : 295200.40655708313\n",
      "for epoch 3 ; training loss : 293874.50341796875\n",
      "for epoch 4 ; training loss : 292540.8145275116\n",
      "for epoch 5 ; training loss : 291197.36417007446\n",
      "for epoch 6 ; training loss : 289842.3477897644\n",
      "for epoch 7 ; training loss : 288476.25592041016\n",
      "for epoch 8 ; training loss : 287097.73509407043\n",
      "for epoch 9 ; training loss : 285702.95097351074\n",
      "for epoch 10 ; training loss : 284289.42234039307\n",
      "for epoch 11 ; training loss : 282858.05809783936\n",
      "for epoch 12 ; training loss : 281406.0220222473\n",
      "for epoch 13 ; training loss : 279932.00067424774\n",
      "for epoch 14 ; training loss : 278429.73521232605\n",
      "for epoch 15 ; training loss : 276900.64159965515\n",
      "for epoch 16 ; training loss : 275342.2863416672\n",
      "for epoch 17 ; training loss : 273754.4563817978\n",
      "for epoch 18 ; training loss : 272136.44572639465\n",
      "for epoch 19 ; training loss : 270487.65720939636\n",
      "for epoch 20 ; training loss : 268807.47371292114\n",
      "for epoch 21 ; training loss : 267093.63868522644\n",
      "for epoch 22 ; training loss : 265344.6566705704\n",
      "for epoch 23 ; training loss : 263563.56752204895\n",
      "for epoch 24 ; training loss : 261750.17025852203\n",
      "for epoch 25 ; training loss : 259905.30829429626\n",
      "for epoch 26 ; training loss : 258026.03628444672\n",
      "for epoch 27 ; training loss : 256111.53521585464\n",
      "for epoch 28 ; training loss : 254162.538169384\n",
      "for epoch 29 ; training loss : 252179.17108345032\n",
      "for epoch 30 ; training loss : 250162.3495669365\n",
      "for epoch 31 ; training loss : 248112.0532245636\n",
      "for epoch 32 ; training loss : 246029.41876125336\n",
      "for epoch 33 ; training loss : 243914.5570383072\n",
      "for epoch 34 ; training loss : 241766.9645462036\n",
      "for epoch 35 ; training loss : 239586.28508520126\n",
      "for epoch 36 ; training loss : 237371.63604211807\n",
      "for epoch 37 ; training loss : 235124.41236305237\n",
      "for epoch 38 ; training loss : 232844.7965669632\n",
      "for epoch 39 ; training loss : 230534.14334106445\n",
      "for epoch 40 ; training loss : 228190.99548697472\n",
      "for epoch 41 ; training loss : 225815.14823007584\n",
      "for epoch 42 ; training loss : 223409.62437915802\n",
      "for epoch 43 ; training loss : 220974.763764143\n",
      "for epoch 44 ; training loss : 218508.81921744347\n",
      "for epoch 45 ; training loss : 216014.76681387424\n",
      "for epoch 46 ; training loss : 213494.00174355507\n",
      "for epoch 47 ; training loss : 210946.12398076057\n",
      "for epoch 48 ; training loss : 208370.5469495654\n",
      "for epoch 49 ; training loss : 205769.49637842178\n",
      "*** Working on model 50 , iter 8 ***\n",
      "for epoch 0 ; training loss : 298473.46033859253\n",
      "for epoch 1 ; training loss : 297300.6587600708\n",
      "for epoch 2 ; training loss : 296117.68527412415\n",
      "for epoch 3 ; training loss : 294924.8873386383\n",
      "for epoch 4 ; training loss : 293722.48491477966\n",
      "for epoch 5 ; training loss : 292509.0102367401\n",
      "for epoch 6 ; training loss : 291284.119222641\n",
      "for epoch 7 ; training loss : 290045.52672576904\n",
      "for epoch 8 ; training loss : 288793.90010261536\n",
      "for epoch 9 ; training loss : 287528.2486858368\n",
      "for epoch 10 ; training loss : 286247.5539569855\n",
      "for epoch 11 ; training loss : 284952.662817955\n",
      "for epoch 12 ; training loss : 283639.8342704773\n",
      "for epoch 13 ; training loss : 282307.7628221512\n",
      "for epoch 14 ; training loss : 280954.3070278168\n",
      "for epoch 15 ; training loss : 279576.80624485016\n",
      "for epoch 16 ; training loss : 278171.4028930664\n",
      "for epoch 17 ; training loss : 276741.00219249725\n",
      "for epoch 18 ; training loss : 275282.5430803299\n",
      "for epoch 19 ; training loss : 273797.89221954346\n",
      "for epoch 20 ; training loss : 272286.8553352356\n",
      "for epoch 21 ; training loss : 270747.8577051163\n",
      "for epoch 22 ; training loss : 269178.86886787415\n",
      "for epoch 23 ; training loss : 267579.09471797943\n",
      "for epoch 24 ; training loss : 265947.35594272614\n",
      "for epoch 25 ; training loss : 264283.7335548401\n",
      "for epoch 26 ; training loss : 262587.8996720314\n",
      "for epoch 27 ; training loss : 260858.98555517197\n",
      "for epoch 28 ; training loss : 259097.02948188782\n",
      "for epoch 29 ; training loss : 257301.81923913956\n",
      "for epoch 30 ; training loss : 255474.0358505249\n",
      "for epoch 31 ; training loss : 253612.25449967384\n",
      "for epoch 32 ; training loss : 251716.93518161774\n",
      "for epoch 33 ; training loss : 249788.90689349174\n",
      "for epoch 34 ; training loss : 247828.47001504898\n",
      "for epoch 35 ; training loss : 245835.97021055222\n",
      "for epoch 36 ; training loss : 243809.905929327\n",
      "for epoch 37 ; training loss : 241751.38336992264\n",
      "for epoch 38 ; training loss : 239661.24484670162\n",
      "for epoch 39 ; training loss : 237538.3837968111\n",
      "for epoch 40 ; training loss : 235383.772051692\n",
      "for epoch 41 ; training loss : 233197.3667295575\n",
      "for epoch 42 ; training loss : 230980.11022096872\n",
      "for epoch 43 ; training loss : 228732.10063004494\n",
      "for epoch 44 ; training loss : 226453.80080628395\n",
      "for epoch 45 ; training loss : 224145.77526780963\n",
      "for epoch 46 ; training loss : 221807.4679645747\n",
      "for epoch 47 ; training loss : 219440.19033350796\n",
      "for epoch 48 ; training loss : 217045.13398950174\n",
      "for epoch 49 ; training loss : 214622.257822264\n",
      "*** Working on model 50 , iter 9 ***\n",
      "for epoch 0 ; training loss : 306184.94487190247\n",
      "for epoch 1 ; training loss : 304909.5796031952\n",
      "for epoch 2 ; training loss : 303634.3377895355\n",
      "for epoch 3 ; training loss : 302358.7838859558\n",
      "for epoch 4 ; training loss : 301082.2185115814\n",
      "for epoch 5 ; training loss : 299804.8134803772\n",
      "for epoch 6 ; training loss : 298525.8046989441\n",
      "for epoch 7 ; training loss : 297243.7544193268\n",
      "for epoch 8 ; training loss : 295956.78921318054\n",
      "for epoch 9 ; training loss : 294664.20310783386\n",
      "for epoch 10 ; training loss : 293363.5786628723\n",
      "for epoch 11 ; training loss : 292053.01456451416\n",
      "for epoch 12 ; training loss : 290730.58509254456\n",
      "for epoch 13 ; training loss : 289396.28831481934\n",
      "for epoch 14 ; training loss : 288046.7994785309\n",
      "for epoch 15 ; training loss : 286681.2478313446\n",
      "for epoch 16 ; training loss : 285298.2051229477\n",
      "for epoch 17 ; training loss : 283895.9648704529\n",
      "for epoch 18 ; training loss : 282474.7283000946\n",
      "for epoch 19 ; training loss : 281034.69535827637\n",
      "for epoch 20 ; training loss : 279575.1910867691\n",
      "for epoch 21 ; training loss : 278095.40785980225\n",
      "for epoch 22 ; training loss : 276594.937374115\n",
      "for epoch 23 ; training loss : 275073.6775112152\n",
      "for epoch 24 ; training loss : 273528.7746953964\n",
      "for epoch 25 ; training loss : 271960.9012889862\n",
      "for epoch 26 ; training loss : 270368.56491184235\n",
      "for epoch 27 ; training loss : 268751.4552001953\n",
      "for epoch 28 ; training loss : 267106.82533454895\n",
      "for epoch 29 ; training loss : 265431.5611195564\n",
      "for epoch 30 ; training loss : 263726.03651571274\n",
      "for epoch 31 ; training loss : 261988.96829319\n",
      "for epoch 32 ; training loss : 260218.85492753983\n",
      "for epoch 33 ; training loss : 258416.63525676727\n",
      "for epoch 34 ; training loss : 256582.6094098091\n",
      "for epoch 35 ; training loss : 254715.60477638245\n",
      "for epoch 36 ; training loss : 252815.58889436722\n",
      "for epoch 37 ; training loss : 250879.81759262085\n",
      "for epoch 38 ; training loss : 248909.63016605377\n",
      "for epoch 39 ; training loss : 246906.77994012833\n",
      "for epoch 40 ; training loss : 244870.139128685\n",
      "for epoch 41 ; training loss : 242800.49715924263\n",
      "for epoch 42 ; training loss : 240696.53286600113\n",
      "for epoch 43 ; training loss : 238559.09777283669\n",
      "for epoch 44 ; training loss : 236388.40249204636\n",
      "for epoch 45 ; training loss : 234184.1858984232\n",
      "for epoch 46 ; training loss : 231945.39447832108\n",
      "for epoch 47 ; training loss : 229672.85783028603\n",
      "for epoch 48 ; training loss : 227367.82767915726\n",
      "for epoch 49 ; training loss : 225031.38969016075\n",
      "*** Working on model 100 , iter 0 ***\n",
      "for epoch 0 ; training loss : 295798.038892746\n",
      "for epoch 1 ; training loss : 293821.573884964\n",
      "for epoch 2 ; training loss : 291840.9560317993\n",
      "for epoch 3 ; training loss : 289837.23428344727\n",
      "for epoch 4 ; training loss : 287803.8953819275\n",
      "for epoch 5 ; training loss : 285736.19342041016\n",
      "for epoch 6 ; training loss : 283629.698387146\n",
      "for epoch 7 ; training loss : 281481.8098564148\n",
      "for epoch 8 ; training loss : 279290.4366388321\n",
      "for epoch 9 ; training loss : 277055.90806770325\n",
      "for epoch 10 ; training loss : 274773.81809329987\n",
      "for epoch 11 ; training loss : 272441.59655952454\n",
      "for epoch 12 ; training loss : 270058.8502006531\n",
      "for epoch 13 ; training loss : 267624.3808455467\n",
      "for epoch 14 ; training loss : 265134.464179039\n",
      "for epoch 15 ; training loss : 262585.61519908905\n",
      "for epoch 16 ; training loss : 259975.79670715332\n",
      "for epoch 17 ; training loss : 257304.8899974823\n",
      "for epoch 18 ; training loss : 254571.38585448265\n",
      "for epoch 19 ; training loss : 251778.34732460976\n",
      "for epoch 20 ; training loss : 248927.12134325504\n",
      "for epoch 21 ; training loss : 246017.56330299377\n",
      "for epoch 22 ; training loss : 243048.1784620881\n",
      "for epoch 23 ; training loss : 240020.0314347148\n",
      "for epoch 24 ; training loss : 236935.0600964576\n",
      "for epoch 25 ; training loss : 233792.8822958842\n",
      "for epoch 26 ; training loss : 230591.57178190304\n",
      "for epoch 27 ; training loss : 227332.41220582277\n",
      "for epoch 28 ; training loss : 224016.30979838967\n",
      "for epoch 29 ; training loss : 220644.43133997917\n",
      "for epoch 30 ; training loss : 217218.68695235997\n",
      "for epoch 31 ; training loss : 213742.0382174184\n",
      "for epoch 32 ; training loss : 210216.74192417413\n",
      "for epoch 33 ; training loss : 206643.913663283\n",
      "for epoch 34 ; training loss : 203026.74968850613\n",
      "for epoch 35 ; training loss : 199366.77816957235\n",
      "for epoch 36 ; training loss : 195666.16652879864\n",
      "for epoch 37 ; training loss : 191928.50016645622\n",
      "for epoch 38 ; training loss : 188157.23092457082\n",
      "for epoch 39 ; training loss : 184355.61607667804\n",
      "for epoch 40 ; training loss : 180528.47799766716\n",
      "for epoch 41 ; training loss : 176679.51683167927\n",
      "for epoch 42 ; training loss : 172812.5366051495\n",
      "for epoch 43 ; training loss : 168930.73958424758\n",
      "for epoch 44 ; training loss : 165036.465956375\n",
      "for epoch 45 ; training loss : 161132.71853668243\n",
      "for epoch 46 ; training loss : 157222.06914198957\n",
      "for epoch 47 ; training loss : 153310.31441973057\n",
      "for epoch 48 ; training loss : 149401.39424149645\n",
      "for epoch 49 ; training loss : 145497.49435899104\n",
      "*** Working on model 100 , iter 1 ***\n",
      "for epoch 0 ; training loss : 296982.6896095276\n",
      "for epoch 1 ; training loss : 294851.93183517456\n",
      "for epoch 2 ; training loss : 292708.0601787567\n",
      "for epoch 3 ; training loss : 290541.63677597046\n",
      "for epoch 4 ; training loss : 288350.14845848083\n",
      "for epoch 5 ; training loss : 286130.28800582886\n",
      "for epoch 6 ; training loss : 283879.3506374359\n",
      "for epoch 7 ; training loss : 281593.78569602966\n",
      "for epoch 8 ; training loss : 279265.5963563919\n",
      "for epoch 9 ; training loss : 276890.4228811264\n",
      "for epoch 10 ; training loss : 274463.1836977005\n",
      "for epoch 11 ; training loss : 271980.77366161346\n",
      "for epoch 12 ; training loss : 269442.4365301132\n",
      "for epoch 13 ; training loss : 266850.0762872696\n",
      "for epoch 14 ; training loss : 264204.5585870743\n",
      "for epoch 15 ; training loss : 261507.42216777802\n",
      "for epoch 16 ; training loss : 258757.09117937088\n",
      "for epoch 17 ; training loss : 255951.195458889\n",
      "for epoch 18 ; training loss : 253091.82016134262\n",
      "for epoch 19 ; training loss : 250180.01415348053\n",
      "for epoch 20 ; training loss : 247216.20538711548\n",
      "for epoch 21 ; training loss : 244201.32688951492\n",
      "for epoch 22 ; training loss : 241137.11305618286\n",
      "for epoch 23 ; training loss : 238023.46286153793\n",
      "for epoch 24 ; training loss : 234858.05980420113\n",
      "for epoch 25 ; training loss : 231642.90990042686\n",
      "for epoch 26 ; training loss : 228377.83027273417\n",
      "for epoch 27 ; training loss : 225063.05976611376\n",
      "for epoch 28 ; training loss : 221699.11867263913\n",
      "for epoch 29 ; training loss : 218288.31521596014\n",
      "for epoch 30 ; training loss : 214831.20644448698\n",
      "for epoch 31 ; training loss : 211327.82616384514\n",
      "for epoch 32 ; training loss : 207782.3984578061\n",
      "for epoch 33 ; training loss : 204196.7027518414\n",
      "for epoch 34 ; training loss : 200572.42809985206\n",
      "for epoch 35 ; training loss : 196912.53587816542\n",
      "for epoch 36 ; training loss : 193217.3523429837\n",
      "for epoch 37 ; training loss : 189487.8008378744\n",
      "for epoch 38 ; training loss : 185728.1527775228\n",
      "for epoch 39 ; training loss : 181941.62997019663\n",
      "for epoch 40 ; training loss : 178130.72829371644\n",
      "for epoch 41 ; training loss : 174297.89579406567\n",
      "for epoch 42 ; training loss : 170446.74337065982\n",
      "for epoch 43 ; training loss : 166581.3569233194\n",
      "for epoch 44 ; training loss : 162705.6041984558\n",
      "for epoch 45 ; training loss : 158821.87838274986\n",
      "for epoch 46 ; training loss : 154932.00350758433\n",
      "for epoch 47 ; training loss : 151041.09601055956\n",
      "for epoch 48 ; training loss : 147152.67797320412\n",
      "for epoch 49 ; training loss : 143271.44123639818\n",
      "*** Working on model 100 , iter 2 ***\n",
      "for epoch 0 ; training loss : 303326.45585250854\n",
      "for epoch 1 ; training loss : 301179.61021995544\n",
      "for epoch 2 ; training loss : 299026.2083301544\n",
      "for epoch 3 ; training loss : 296867.15484046936\n",
      "for epoch 4 ; training loss : 294702.67552375793\n",
      "for epoch 5 ; training loss : 292531.3271846771\n",
      "for epoch 6 ; training loss : 290347.0095272064\n",
      "for epoch 7 ; training loss : 288146.68626499176\n",
      "for epoch 8 ; training loss : 285928.0346632004\n",
      "for epoch 9 ; training loss : 283686.23330020905\n",
      "for epoch 10 ; training loss : 281417.25895500183\n",
      "for epoch 11 ; training loss : 279117.61096286774\n",
      "for epoch 12 ; training loss : 276786.92650699615\n",
      "for epoch 13 ; training loss : 274421.55278110504\n",
      "for epoch 14 ; training loss : 272020.7319698334\n",
      "for epoch 15 ; training loss : 269582.8207540512\n",
      "for epoch 16 ; training loss : 267104.9436364174\n",
      "for epoch 17 ; training loss : 264585.139295578\n",
      "for epoch 18 ; training loss : 262023.6411807537\n",
      "for epoch 19 ; training loss : 259417.83807945251\n",
      "for epoch 20 ; training loss : 256766.95027065277\n",
      "for epoch 21 ; training loss : 254069.14621961117\n",
      "for epoch 22 ; training loss : 251322.169652462\n",
      "for epoch 23 ; training loss : 248524.42431092262\n",
      "for epoch 24 ; training loss : 245675.77212190628\n",
      "for epoch 25 ; training loss : 242774.77099481225\n",
      "for epoch 26 ; training loss : 239818.84219694138\n",
      "for epoch 27 ; training loss : 236808.1505995281\n",
      "for epoch 28 ; training loss : 233742.18713814998\n",
      "for epoch 29 ; training loss : 230621.49556249753\n",
      "for epoch 30 ; training loss : 227449.25242368132\n",
      "for epoch 31 ; training loss : 224225.2370037064\n",
      "for epoch 32 ; training loss : 220950.78875708953\n",
      "for epoch 33 ; training loss : 217622.99639604834\n",
      "for epoch 34 ; training loss : 214241.91207868792\n",
      "for epoch 35 ; training loss : 210812.6382310763\n",
      "for epoch 36 ; training loss : 207336.79173505306\n",
      "for epoch 37 ; training loss : 203814.58588184044\n",
      "for epoch 38 ; training loss : 200248.25809681648\n",
      "for epoch 39 ; training loss : 196640.3646078054\n",
      "for epoch 40 ; training loss : 192992.75994868577\n",
      "for epoch 41 ; training loss : 189308.6048192978\n",
      "for epoch 42 ; training loss : 185590.91360014048\n",
      "for epoch 43 ; training loss : 181843.86214901973\n",
      "for epoch 44 ; training loss : 178072.01293462515\n",
      "for epoch 45 ; training loss : 174276.04762354493\n",
      "for epoch 46 ; training loss : 170458.21637284756\n",
      "for epoch 47 ; training loss : 166620.81090775132\n",
      "for epoch 48 ; training loss : 162767.4635655582\n",
      "for epoch 49 ; training loss : 158903.0214991346\n",
      "*** Working on model 100 , iter 3 ***\n",
      "for epoch 0 ; training loss : 294190.0923862457\n",
      "for epoch 1 ; training loss : 291976.2927417755\n",
      "for epoch 2 ; training loss : 289730.25030326843\n",
      "for epoch 3 ; training loss : 287449.60753822327\n",
      "for epoch 4 ; training loss : 285135.81408309937\n",
      "for epoch 5 ; training loss : 282784.1279449463\n",
      "for epoch 6 ; training loss : 280390.20564460754\n",
      "for epoch 7 ; training loss : 277950.5468711853\n",
      "for epoch 8 ; training loss : 275465.0086221695\n",
      "for epoch 9 ; training loss : 272935.0403842926\n",
      "for epoch 10 ; training loss : 270356.96456336975\n",
      "for epoch 11 ; training loss : 267728.2601118088\n",
      "for epoch 12 ; training loss : 265050.6602334976\n",
      "for epoch 13 ; training loss : 262322.8743867874\n",
      "for epoch 14 ; training loss : 259544.0259361267\n",
      "for epoch 15 ; training loss : 256715.44905281067\n",
      "for epoch 16 ; training loss : 253837.10172462463\n",
      "for epoch 17 ; training loss : 250907.67724370956\n",
      "for epoch 18 ; training loss : 247925.8712363243\n",
      "for epoch 19 ; training loss : 244892.478454113\n",
      "for epoch 20 ; training loss : 241805.62202262878\n",
      "for epoch 21 ; training loss : 238664.6231329441\n",
      "for epoch 22 ; training loss : 235470.38712835312\n",
      "for epoch 23 ; training loss : 232216.9733362198\n",
      "for epoch 24 ; training loss : 228906.39473581314\n",
      "for epoch 25 ; training loss : 225537.29389345646\n",
      "for epoch 26 ; training loss : 222110.00155973434\n",
      "for epoch 27 ; training loss : 218625.20943164825\n",
      "for epoch 28 ; training loss : 215086.17479890585\n",
      "for epoch 29 ; training loss : 211495.00173333287\n",
      "for epoch 30 ; training loss : 207854.3555584699\n",
      "for epoch 31 ; training loss : 204164.49166341126\n",
      "for epoch 32 ; training loss : 200429.027574569\n",
      "for epoch 33 ; training loss : 196649.45600507758\n",
      "for epoch 34 ; training loss : 192829.18700460065\n",
      "for epoch 35 ; training loss : 188972.82187553495\n",
      "for epoch 36 ; training loss : 185082.30968643725\n",
      "for epoch 37 ; training loss : 181159.88389979303\n",
      "for epoch 38 ; training loss : 177211.46069391072\n",
      "for epoch 39 ; training loss : 173238.54933037888\n",
      "for epoch 40 ; training loss : 169248.73925930867\n",
      "for epoch 41 ; training loss : 165243.74949363247\n",
      "for epoch 42 ; training loss : 161226.2653945312\n",
      "for epoch 43 ; training loss : 157201.0478515895\n",
      "for epoch 44 ; training loss : 153173.64864730835\n",
      "for epoch 45 ; training loss : 149148.80194760114\n",
      "for epoch 46 ; training loss : 145130.89379549026\n",
      "for epoch 47 ; training loss : 141123.80842901394\n",
      "for epoch 48 ; training loss : 137133.08154923614\n",
      "for epoch 49 ; training loss : 133163.61129231006\n",
      "*** Working on model 100 , iter 4 ***\n",
      "for epoch 0 ; training loss : 304015.83530044556\n",
      "for epoch 1 ; training loss : 302021.38796424866\n",
      "for epoch 2 ; training loss : 300022.3369998932\n",
      "for epoch 3 ; training loss : 298011.0843343735\n",
      "for epoch 4 ; training loss : 295984.7346382141\n",
      "for epoch 5 ; training loss : 293937.1385316849\n",
      "for epoch 6 ; training loss : 291864.65358638763\n",
      "for epoch 7 ; training loss : 289763.48895072937\n",
      "for epoch 8 ; training loss : 287627.33609580994\n",
      "for epoch 9 ; training loss : 285457.4064273834\n",
      "for epoch 10 ; training loss : 283249.08600234985\n",
      "for epoch 11 ; training loss : 280998.9108223915\n",
      "for epoch 12 ; training loss : 278705.4449687004\n",
      "for epoch 13 ; training loss : 276367.97582292557\n",
      "for epoch 14 ; training loss : 273984.33440470695\n",
      "for epoch 15 ; training loss : 271550.3246912956\n",
      "for epoch 16 ; training loss : 269061.5128209591\n",
      "for epoch 17 ; training loss : 266514.1068575382\n",
      "for epoch 18 ; training loss : 263909.4060639143\n",
      "for epoch 19 ; training loss : 261242.75013828278\n",
      "for epoch 20 ; training loss : 258513.87668716908\n",
      "for epoch 21 ; training loss : 255723.25406843424\n",
      "for epoch 22 ; training loss : 252869.30833652616\n",
      "for epoch 23 ; training loss : 249950.68719655275\n",
      "for epoch 24 ; training loss : 246966.00556697696\n",
      "for epoch 25 ; training loss : 243917.74969979376\n",
      "for epoch 26 ; training loss : 240804.54900214402\n",
      "for epoch 27 ; training loss : 237628.83481188118\n",
      "for epoch 28 ; training loss : 234390.019286111\n",
      "for epoch 29 ; training loss : 231087.1320451796\n",
      "for epoch 30 ; training loss : 227724.5524059534\n",
      "for epoch 31 ; training loss : 224303.19030159712\n",
      "for epoch 32 ; training loss : 220824.44346377254\n",
      "for epoch 33 ; training loss : 217289.91498921812\n",
      "for epoch 34 ; training loss : 213703.62797281146\n",
      "for epoch 35 ; training loss : 210066.4156088084\n",
      "for epoch 36 ; training loss : 206380.74983947328\n",
      "for epoch 37 ; training loss : 202647.26628285274\n",
      "for epoch 38 ; training loss : 198870.2150331959\n",
      "for epoch 39 ; training loss : 195053.73701049387\n",
      "for epoch 40 ; training loss : 191199.52580600977\n",
      "for epoch 41 ; training loss : 187313.3966524899\n",
      "for epoch 42 ; training loss : 183397.15020015836\n",
      "for epoch 43 ; training loss : 179450.65297663212\n",
      "for epoch 44 ; training loss : 175478.7651327923\n",
      "for epoch 45 ; training loss : 171484.60902054422\n",
      "for epoch 46 ; training loss : 167473.46593923483\n",
      "for epoch 47 ; training loss : 163451.72191053815\n",
      "for epoch 48 ; training loss : 159422.28768276967\n",
      "for epoch 49 ; training loss : 155388.27349673957\n",
      "*** Working on model 100 , iter 5 ***\n",
      "for epoch 0 ; training loss : 300499.64433288574\n",
      "for epoch 1 ; training loss : 298428.41610336304\n",
      "for epoch 2 ; training loss : 296339.3474330902\n",
      "for epoch 3 ; training loss : 294222.21796798706\n",
      "for epoch 4 ; training loss : 292078.5673046112\n",
      "for epoch 5 ; training loss : 289907.5390739441\n",
      "for epoch 6 ; training loss : 287706.4903430939\n",
      "for epoch 7 ; training loss : 285474.7831983566\n",
      "for epoch 8 ; training loss : 283208.86986351013\n",
      "for epoch 9 ; training loss : 280906.6381845474\n",
      "for epoch 10 ; training loss : 278567.1278553009\n",
      "for epoch 11 ; training loss : 276190.0005750656\n",
      "for epoch 12 ; training loss : 273774.6834306717\n",
      "for epoch 13 ; training loss : 271318.956738472\n",
      "for epoch 14 ; training loss : 268822.27952575684\n",
      "for epoch 15 ; training loss : 266282.60191726685\n",
      "for epoch 16 ; training loss : 263698.8194131851\n",
      "for epoch 17 ; training loss : 261068.63926267624\n",
      "for epoch 18 ; training loss : 258390.13414621353\n",
      "for epoch 19 ; training loss : 255662.66839027405\n",
      "for epoch 20 ; training loss : 252886.23866820335\n",
      "for epoch 21 ; training loss : 250059.14527368546\n",
      "for epoch 22 ; training loss : 247181.58614873886\n",
      "for epoch 23 ; training loss : 244254.60985136032\n",
      "for epoch 24 ; training loss : 241276.09280097485\n",
      "for epoch 25 ; training loss : 238246.81847965717\n",
      "for epoch 26 ; training loss : 235167.86513090134\n",
      "for epoch 27 ; training loss : 232038.19619446993\n",
      "for epoch 28 ; training loss : 228860.778783679\n",
      "for epoch 29 ; training loss : 225634.9849435091\n",
      "for epoch 30 ; training loss : 222362.22296586633\n",
      "for epoch 31 ; training loss : 219044.33502194285\n",
      "for epoch 32 ; training loss : 215682.03447915614\n",
      "for epoch 33 ; training loss : 212274.15471523255\n",
      "for epoch 34 ; training loss : 208820.06559243752\n",
      "for epoch 35 ; training loss : 205323.80117614288\n",
      "for epoch 36 ; training loss : 201789.71766296402\n",
      "for epoch 37 ; training loss : 198219.6455166638\n",
      "for epoch 38 ; training loss : 194615.2355453968\n",
      "for epoch 39 ; training loss : 190977.28703939915\n",
      "for epoch 40 ; training loss : 187307.15822018683\n",
      "for epoch 41 ; training loss : 183609.72422454506\n",
      "for epoch 42 ; training loss : 179890.19142446955\n",
      "for epoch 43 ; training loss : 176149.6976861395\n",
      "for epoch 44 ; training loss : 172392.2586764805\n",
      "for epoch 45 ; training loss : 168620.21436753124\n",
      "for epoch 46 ; training loss : 164835.12294317968\n",
      "for epoch 47 ; training loss : 161040.69283583586\n",
      "for epoch 48 ; training loss : 157243.95104212686\n",
      "for epoch 49 ; training loss : 153447.33610968664\n",
      "*** Working on model 100 , iter 6 ***\n",
      "for epoch 0 ; training loss : 293109.24756240845\n",
      "for epoch 1 ; training loss : 290925.83046913147\n",
      "for epoch 2 ; training loss : 288721.35726451874\n",
      "for epoch 3 ; training loss : 286490.82224178314\n",
      "for epoch 4 ; training loss : 284236.7448453903\n",
      "for epoch 5 ; training loss : 281958.8566598892\n",
      "for epoch 6 ; training loss : 279653.6184768677\n",
      "for epoch 7 ; training loss : 277317.64659690857\n",
      "for epoch 8 ; training loss : 274948.87284851074\n",
      "for epoch 9 ; training loss : 272543.95898866653\n",
      "for epoch 10 ; training loss : 270099.9021205902\n",
      "for epoch 11 ; training loss : 267615.7811422348\n",
      "for epoch 12 ; training loss : 265090.65602350235\n",
      "for epoch 13 ; training loss : 262523.90654349327\n",
      "for epoch 14 ; training loss : 259914.07521367073\n",
      "for epoch 15 ; training loss : 257261.01678538322\n",
      "for epoch 16 ; training loss : 254565.2876741886\n",
      "for epoch 17 ; training loss : 251823.94738328457\n",
      "for epoch 18 ; training loss : 249034.69771790504\n",
      "for epoch 19 ; training loss : 246195.88461846113\n",
      "for epoch 20 ; training loss : 243304.82198095322\n",
      "for epoch 21 ; training loss : 240362.27963891625\n",
      "for epoch 22 ; training loss : 237368.52853826806\n",
      "for epoch 23 ; training loss : 234322.94356113812\n",
      "for epoch 24 ; training loss : 231225.12214051373\n",
      "for epoch 25 ; training loss : 228074.64960488677\n",
      "for epoch 26 ; training loss : 224875.33840727806\n",
      "for epoch 27 ; training loss : 221626.4820447713\n",
      "for epoch 28 ; training loss : 218329.79332067817\n",
      "for epoch 29 ; training loss : 214986.57568259444\n",
      "for epoch 30 ; training loss : 211597.37185780867\n",
      "for epoch 31 ; training loss : 208162.380070623\n",
      "for epoch 32 ; training loss : 204683.0183113981\n",
      "for epoch 33 ; training loss : 201160.63164267782\n",
      "for epoch 34 ; training loss : 197595.03058108687\n",
      "for epoch 35 ; training loss : 193989.1858113557\n",
      "for epoch 36 ; training loss : 190347.72292624973\n",
      "for epoch 37 ; training loss : 186672.3538493201\n",
      "for epoch 38 ; training loss : 182966.14544570912\n",
      "for epoch 39 ; training loss : 179231.4001466371\n",
      "for epoch 40 ; training loss : 175472.76369014755\n",
      "for epoch 41 ; training loss : 171692.22909476934\n",
      "for epoch 42 ; training loss : 167891.39266239107\n",
      "for epoch 43 ; training loss : 164073.72857167572\n",
      "for epoch 44 ; training loss : 160242.46695770044\n",
      "for epoch 45 ; training loss : 156402.5311529755\n",
      "for epoch 46 ; training loss : 152557.09058282617\n",
      "for epoch 47 ; training loss : 148710.05316185456\n",
      "for epoch 48 ; training loss : 144866.29263765004\n",
      "for epoch 49 ; training loss : 141029.08819986554\n",
      "*** Working on model 100 , iter 7 ***\n",
      "for epoch 0 ; training loss : 303636.79347610474\n",
      "for epoch 1 ; training loss : 301561.9463939667\n",
      "for epoch 2 ; training loss : 299507.3897476196\n",
      "for epoch 3 ; training loss : 297450.187335968\n",
      "for epoch 4 ; training loss : 295388.11006736755\n",
      "for epoch 5 ; training loss : 293318.3825626373\n",
      "for epoch 6 ; training loss : 291239.94907569885\n",
      "for epoch 7 ; training loss : 289150.17791843414\n",
      "for epoch 8 ; training loss : 287044.11664295197\n",
      "for epoch 9 ; training loss : 284914.68587875366\n",
      "for epoch 10 ; training loss : 282760.0493488312\n",
      "for epoch 11 ; training loss : 280577.91283369064\n",
      "for epoch 12 ; training loss : 278366.54753160477\n",
      "for epoch 13 ; training loss : 276125.15551137924\n",
      "for epoch 14 ; training loss : 273851.30620861053\n",
      "for epoch 15 ; training loss : 271543.59695386887\n",
      "for epoch 16 ; training loss : 269199.57012438774\n",
      "for epoch 17 ; training loss : 266818.22798419\n",
      "for epoch 18 ; training loss : 264398.1709551811\n",
      "for epoch 19 ; training loss : 261938.52667617798\n",
      "for epoch 20 ; training loss : 259438.19380646944\n",
      "for epoch 21 ; training loss : 256897.04933685064\n",
      "for epoch 22 ; training loss : 254312.1965456307\n",
      "for epoch 23 ; training loss : 251683.4326364249\n",
      "for epoch 24 ; training loss : 249010.51167636923\n",
      "for epoch 25 ; training loss : 246293.14541014144\n",
      "for epoch 26 ; training loss : 243527.95251479\n",
      "for epoch 27 ; training loss : 240715.64866907895\n",
      "for epoch 28 ; training loss : 237856.86229852354\n",
      "for epoch 29 ; training loss : 234948.93849754706\n",
      "for epoch 30 ; training loss : 231991.0534046553\n",
      "for epoch 31 ; training loss : 228981.939026203\n",
      "for epoch 32 ; training loss : 225922.82011011243\n",
      "for epoch 33 ; training loss : 222815.3565608263\n",
      "for epoch 34 ; training loss : 219661.90116631985\n",
      "for epoch 35 ; training loss : 216463.6408343166\n",
      "for epoch 36 ; training loss : 213220.9140777439\n",
      "for epoch 37 ; training loss : 209933.41875287704\n",
      "for epoch 38 ; training loss : 206604.3922785006\n",
      "for epoch 39 ; training loss : 203236.23977770284\n",
      "for epoch 40 ; training loss : 199829.67341775747\n",
      "for epoch 41 ; training loss : 196385.06423715875\n",
      "for epoch 42 ; training loss : 192905.9072145049\n",
      "for epoch 43 ; training loss : 189393.13118315046\n",
      "for epoch 44 ; training loss : 185848.11291012884\n",
      "for epoch 45 ; training loss : 182274.59538856178\n",
      "for epoch 46 ; training loss : 178674.4171610952\n",
      "for epoch 47 ; training loss : 175049.98784192593\n",
      "for epoch 48 ; training loss : 171404.38879925758\n",
      "for epoch 49 ; training loss : 167737.87509511667\n",
      "*** Working on model 100 , iter 8 ***\n",
      "for epoch 0 ; training loss : 295289.7326030731\n",
      "for epoch 1 ; training loss : 293129.17534065247\n",
      "for epoch 2 ; training loss : 290939.3607635498\n",
      "for epoch 3 ; training loss : 288714.65476989746\n",
      "for epoch 4 ; training loss : 286455.19438552856\n",
      "for epoch 5 ; training loss : 284160.833650589\n",
      "for epoch 6 ; training loss : 281830.5719718933\n",
      "for epoch 7 ; training loss : 279463.1637458801\n",
      "for epoch 8 ; training loss : 277054.51080513\n",
      "for epoch 9 ; training loss : 274601.8924846649\n",
      "for epoch 10 ; training loss : 272102.60023498535\n",
      "for epoch 11 ; training loss : 269554.21781921387\n",
      "for epoch 12 ; training loss : 266954.2249546051\n",
      "for epoch 13 ; training loss : 264301.49078559875\n",
      "for epoch 14 ; training loss : 261593.32304000854\n",
      "for epoch 15 ; training loss : 258828.31973838806\n",
      "for epoch 16 ; training loss : 256004.94271564484\n",
      "for epoch 17 ; training loss : 253123.39422893524\n",
      "for epoch 18 ; training loss : 250183.41524410248\n",
      "for epoch 19 ; training loss : 247187.6064195633\n",
      "for epoch 20 ; training loss : 244136.93995428085\n",
      "for epoch 21 ; training loss : 241030.58772277832\n",
      "for epoch 22 ; training loss : 237870.05258750916\n",
      "for epoch 23 ; training loss : 234654.54760074615\n",
      "for epoch 24 ; training loss : 231385.97137641907\n",
      "for epoch 25 ; training loss : 228066.4923505783\n",
      "for epoch 26 ; training loss : 224697.708496809\n",
      "for epoch 27 ; training loss : 221281.18569397926\n",
      "for epoch 28 ; training loss : 217817.9539065361\n",
      "for epoch 29 ; training loss : 214307.69149428606\n",
      "for epoch 30 ; training loss : 210752.40934854746\n",
      "for epoch 31 ; training loss : 207154.86909726262\n",
      "for epoch 32 ; training loss : 203514.5530283749\n",
      "for epoch 33 ; training loss : 199831.41721973196\n",
      "for epoch 34 ; training loss : 196108.9351689763\n",
      "for epoch 35 ; training loss : 192348.65961731225\n",
      "for epoch 36 ; training loss : 188556.20746819163\n",
      "for epoch 37 ; training loss : 184734.14747480303\n",
      "for epoch 38 ; training loss : 180884.73610708117\n",
      "for epoch 39 ; training loss : 177011.23956213892\n",
      "for epoch 40 ; training loss : 173116.95390527323\n",
      "for epoch 41 ; training loss : 169208.34179192712\n",
      "for epoch 42 ; training loss : 165287.490453667\n",
      "for epoch 43 ; training loss : 161359.16476317495\n",
      "for epoch 44 ; training loss : 157426.04446372762\n",
      "for epoch 45 ; training loss : 153491.58627353585\n",
      "for epoch 46 ; training loss : 149562.05360405988\n",
      "for epoch 47 ; training loss : 145639.36254091002\n",
      "for epoch 48 ; training loss : 141728.4536562804\n",
      "for epoch 49 ; training loss : 137832.17192462762\n",
      "*** Working on model 100 , iter 9 ***\n",
      "for epoch 0 ; training loss : 298534.0872364044\n",
      "for epoch 1 ; training loss : 296572.53259658813\n",
      "for epoch 2 ; training loss : 294608.6531791687\n",
      "for epoch 3 ; training loss : 292635.78231048584\n",
      "for epoch 4 ; training loss : 290649.66612815857\n",
      "for epoch 5 ; training loss : 288646.7842578888\n",
      "for epoch 6 ; training loss : 286622.0909309387\n",
      "for epoch 7 ; training loss : 284571.09737586975\n",
      "for epoch 8 ; training loss : 282488.59458732605\n",
      "for epoch 9 ; training loss : 280370.500418663\n",
      "for epoch 10 ; training loss : 278211.20612716675\n",
      "for epoch 11 ; training loss : 276011.78884506226\n",
      "for epoch 12 ; training loss : 273770.1063699722\n",
      "for epoch 13 ; training loss : 271483.5652899742\n",
      "for epoch 14 ; training loss : 269151.09064388275\n",
      "for epoch 15 ; training loss : 266771.13219451904\n",
      "for epoch 16 ; training loss : 264341.2815055847\n",
      "for epoch 17 ; training loss : 261858.96003484726\n",
      "for epoch 18 ; training loss : 259321.7452123165\n",
      "for epoch 19 ; training loss : 256728.23104071617\n",
      "for epoch 20 ; training loss : 254078.4085252285\n",
      "for epoch 21 ; training loss : 251371.23226356506\n",
      "for epoch 22 ; training loss : 248609.79008364677\n",
      "for epoch 23 ; training loss : 245795.9878950119\n",
      "for epoch 24 ; training loss : 242925.3745406866\n",
      "for epoch 25 ; training loss : 239999.77558755875\n",
      "for epoch 26 ; training loss : 237022.09515678883\n",
      "for epoch 27 ; training loss : 233990.7874390036\n",
      "for epoch 28 ; training loss : 230905.97322987393\n",
      "for epoch 29 ; training loss : 227769.17933027016\n",
      "for epoch 30 ; training loss : 224579.63150730915\n",
      "for epoch 31 ; training loss : 221338.59021941572\n",
      "for epoch 32 ; training loss : 218046.25244253874\n",
      "for epoch 33 ; training loss : 214702.20261749253\n",
      "for epoch 34 ; training loss : 211308.70267911023\n",
      "for epoch 35 ; training loss : 207868.48281292338\n",
      "for epoch 36 ; training loss : 204384.32511921972\n",
      "for epoch 37 ; training loss : 200856.85549621284\n",
      "for epoch 38 ; training loss : 197288.92683397979\n",
      "for epoch 39 ; training loss : 193684.6968663477\n",
      "for epoch 40 ; training loss : 190046.85713279247\n",
      "for epoch 41 ; training loss : 186376.9305801466\n",
      "for epoch 42 ; training loss : 182676.23823358305\n",
      "for epoch 43 ; training loss : 178949.70852889004\n",
      "for epoch 44 ; training loss : 175200.07125260495\n",
      "for epoch 45 ; training loss : 171432.05672879517\n",
      "for epoch 46 ; training loss : 167648.81198380142\n",
      "for epoch 47 ; training loss : 163854.14420469198\n",
      "for epoch 48 ; training loss : 160051.02711246256\n",
      "for epoch 49 ; training loss : 156243.45136466622\n",
      "*** Working on model 500 , iter 0 ***\n",
      "for epoch 0 ; training loss : 293313.33369731903\n",
      "for epoch 1 ; training loss : 284702.4156990051\n",
      "for epoch 2 ; training loss : 276143.9869647026\n",
      "for epoch 3 ; training loss : 267601.87131774426\n",
      "for epoch 4 ; training loss : 259056.4514681399\n",
      "for epoch 5 ; training loss : 250487.17287463695\n",
      "for epoch 6 ; training loss : 241882.46468184888\n",
      "for epoch 7 ; training loss : 233238.58352465602\n",
      "for epoch 8 ; training loss : 224554.37138765492\n",
      "for epoch 9 ; training loss : 215835.42305521667\n",
      "for epoch 10 ; training loss : 207087.16949798784\n",
      "for epoch 11 ; training loss : 198313.98566722905\n",
      "for epoch 12 ; training loss : 189528.25673990697\n",
      "for epoch 13 ; training loss : 180740.68015351903\n",
      "for epoch 14 ; training loss : 171966.59853203507\n",
      "for epoch 15 ; training loss : 163224.19726383686\n",
      "for epoch 16 ; training loss : 154537.21318016388\n",
      "for epoch 17 ; training loss : 145932.10207967833\n",
      "for epoch 18 ; training loss : 137438.7033111768\n",
      "for epoch 19 ; training loss : 129090.3276312165\n",
      "for epoch 20 ; training loss : 120920.0867375657\n",
      "for epoch 21 ; training loss : 112963.68080121279\n",
      "for epoch 22 ; training loss : 105258.12238124758\n",
      "for epoch 23 ; training loss : 97838.96324605055\n",
      "for epoch 24 ; training loss : 90739.54180779743\n",
      "for epoch 25 ; training loss : 83990.74044703116\n",
      "for epoch 26 ; training loss : 77620.13257850369\n",
      "for epoch 27 ; training loss : 71652.28439857173\n",
      "for epoch 28 ; training loss : 66105.32065465604\n",
      "for epoch 29 ; training loss : 60991.84437021939\n",
      "for epoch 30 ; training loss : 56317.89817871309\n",
      "for epoch 31 ; training loss : 52080.464746517595\n",
      "for epoch 32 ; training loss : 48269.627383394865\n",
      "for epoch 33 ; training loss : 44865.69786989868\n",
      "for epoch 34 ; training loss : 41841.426183564196\n",
      "for epoch 35 ; training loss : 39162.72429874161\n",
      "for epoch 36 ; training loss : 36795.206961767195\n",
      "for epoch 37 ; training loss : 34702.38314958055\n",
      "for epoch 38 ; training loss : 32848.73195126827\n",
      "for epoch 39 ; training loss : 31201.72818870485\n",
      "for epoch 40 ; training loss : 29730.896101444552\n",
      "for epoch 41 ; training loss : 28410.20551460347\n",
      "for epoch 42 ; training loss : 27217.089008132345\n",
      "for epoch 43 ; training loss : 26132.337220334535\n",
      "for epoch 44 ; training loss : 25140.122631177655\n",
      "for epoch 45 ; training loss : 24227.725516694973\n",
      "for epoch 46 ; training loss : 23384.938665481866\n",
      "for epoch 47 ; training loss : 22602.906561857893\n",
      "for epoch 48 ; training loss : 21874.691004582393\n",
      "for epoch 49 ; training loss : 21194.731254503073\n",
      "*** Working on model 500 , iter 1 ***\n",
      "for epoch 0 ; training loss : 295355.5873775482\n",
      "for epoch 1 ; training loss : 287113.89850854874\n",
      "for epoch 2 ; training loss : 278929.5101428032\n",
      "for epoch 3 ; training loss : 270772.4374015331\n",
      "for epoch 4 ; training loss : 262627.47476375103\n",
      "for epoch 5 ; training loss : 254471.34564536205\n",
      "for epoch 6 ; training loss : 246279.52869218588\n",
      "for epoch 7 ; training loss : 238035.72136873007\n",
      "for epoch 8 ; training loss : 229732.15353545547\n",
      "for epoch 9 ; training loss : 221367.52384857833\n",
      "for epoch 10 ; training loss : 212942.96727132227\n",
      "for epoch 11 ; training loss : 204463.19945832342\n",
      "for epoch 12 ; training loss : 195933.51633163914\n",
      "for epoch 13 ; training loss : 187361.48501090938\n",
      "for epoch 14 ; training loss : 178763.2663418292\n",
      "for epoch 15 ; training loss : 170158.94464406115\n",
      "for epoch 16 ; training loss : 161570.5705113653\n",
      "for epoch 17 ; training loss : 153024.8174705119\n",
      "for epoch 18 ; training loss : 144550.15086650103\n",
      "for epoch 19 ; training loss : 136173.66246068466\n",
      "for epoch 20 ; training loss : 127931.68071939796\n",
      "for epoch 21 ; training loss : 119858.1631036587\n",
      "for epoch 22 ; training loss : 111989.70144393016\n",
      "for epoch 23 ; training loss : 104362.2693964048\n",
      "for epoch 24 ; training loss : 97008.72725531925\n",
      "for epoch 25 ; training loss : 89964.01476563048\n",
      "for epoch 26 ; training loss : 83260.7347193347\n",
      "for epoch 27 ; training loss : 76927.32828594837\n",
      "for epoch 28 ; training loss : 70988.36204374884\n",
      "for epoch 29 ; training loss : 65464.027192889014\n",
      "for epoch 30 ; training loss : 60367.33601348262\n",
      "for epoch 31 ; training loss : 55704.45212381985\n",
      "for epoch 32 ; training loss : 51474.751665171614\n",
      "for epoch 33 ; training loss : 47668.33259164856\n",
      "for epoch 34 ; training loss : 44267.96901204784\n",
      "for epoch 35 ; training loss : 41248.95925819228\n",
      "for epoch 36 ; training loss : 38579.439614495015\n",
      "for epoch 37 ; training loss : 36225.99048456736\n",
      "for epoch 38 ; training loss : 34152.42067835801\n",
      "for epoch 39 ; training loss : 32323.2839295133\n",
      "for epoch 40 ; training loss : 30704.841616034988\n",
      "for epoch 41 ; training loss : 29266.557191903525\n",
      "for epoch 42 ; training loss : 27981.46293104702\n",
      "for epoch 43 ; training loss : 26825.990215508733\n",
      "for epoch 44 ; training loss : 25780.327253170777\n",
      "for epoch 45 ; training loss : 24827.73522699089\n",
      "for epoch 46 ; training loss : 23954.106472482446\n",
      "for epoch 47 ; training loss : 23148.43936947521\n",
      "for epoch 48 ; training loss : 22402.2832698469\n",
      "for epoch 49 ; training loss : 21708.447434716247\n",
      "*** Working on model 500 , iter 2 ***\n",
      "for epoch 0 ; training loss : 291447.5709133148\n",
      "for epoch 1 ; training loss : 283035.3980178833\n",
      "for epoch 2 ; training loss : 274683.15513420105\n",
      "for epoch 3 ; training loss : 266336.6240038872\n",
      "for epoch 4 ; training loss : 257978.2370789051\n",
      "for epoch 5 ; training loss : 249593.99274909496\n",
      "for epoch 6 ; training loss : 241171.45332506206\n",
      "for epoch 7 ; training loss : 232701.0986814201\n",
      "for epoch 8 ; training loss : 224175.89417434856\n",
      "for epoch 9 ; training loss : 215594.29820232093\n",
      "for epoch 10 ; training loss : 206956.62913589552\n",
      "for epoch 11 ; training loss : 198269.88514342904\n",
      "for epoch 12 ; training loss : 189544.71186286584\n",
      "for epoch 13 ; training loss : 180792.66211158596\n",
      "for epoch 14 ; training loss : 172032.60084451828\n",
      "for epoch 15 ; training loss : 163289.1907615047\n",
      "for epoch 16 ; training loss : 154590.793171877\n",
      "for epoch 17 ; training loss : 145966.45754995337\n",
      "for epoch 18 ; training loss : 137448.60788509576\n",
      "for epoch 19 ; training loss : 129072.69616785168\n",
      "for epoch 20 ; training loss : 120875.82057181373\n",
      "for epoch 21 ; training loss : 112894.85267918021\n",
      "for epoch 22 ; training loss : 105167.50586781558\n",
      "for epoch 23 ; training loss : 97727.09332685173\n",
      "for epoch 24 ; training loss : 90608.19586969377\n",
      "for epoch 25 ; training loss : 83843.8635394359\n",
      "for epoch 26 ; training loss : 77461.05199725064\n",
      "for epoch 27 ; training loss : 71484.87095047534\n",
      "for epoch 28 ; training loss : 65933.6574903849\n",
      "for epoch 29 ; training loss : 60818.467342912685\n",
      "for epoch 30 ; training loss : 56144.305524561394\n",
      "for epoch 31 ; training loss : 51907.49745509331\n",
      "for epoch 32 ; training loss : 48096.63256154573\n",
      "for epoch 33 ; training loss : 44691.03853894188\n",
      "for epoch 34 ; training loss : 41664.018262745\n",
      "for epoch 35 ; training loss : 38983.47747495753\n",
      "for epoch 36 ; training loss : 36614.895293251146\n",
      "for epoch 37 ; training loss : 34522.46450281399\n",
      "for epoch 38 ; training loss : 32670.56989127481\n",
      "for epoch 39 ; training loss : 31026.040077999824\n",
      "for epoch 40 ; training loss : 29559.368541332897\n",
      "for epoch 41 ; training loss : 28244.10430057341\n",
      "for epoch 42 ; training loss : 27057.736554474657\n",
      "for epoch 43 ; training loss : 25981.80125999749\n",
      "for epoch 44 ; training loss : 24999.609727612828\n",
      "for epoch 45 ; training loss : 24097.85247706587\n",
      "for epoch 46 ; training loss : 23265.898587423377\n",
      "for epoch 47 ; training loss : 22494.777497888077\n",
      "for epoch 48 ; training loss : 21777.29898146345\n",
      "for epoch 49 ; training loss : 21107.532514745602\n",
      "*** Working on model 500 , iter 3 ***\n",
      "for epoch 0 ; training loss : 299351.1422843933\n",
      "for epoch 1 ; training loss : 290597.1231174469\n",
      "for epoch 2 ; training loss : 281949.79789066315\n",
      "for epoch 3 ; training loss : 273360.54815864563\n",
      "for epoch 4 ; training loss : 264803.8000268936\n",
      "for epoch 5 ; training loss : 256253.50589209795\n",
      "for epoch 6 ; training loss : 247685.7615436148\n",
      "for epoch 7 ; training loss : 239085.73520949483\n",
      "for epoch 8 ; training loss : 230446.44829915464\n",
      "for epoch 9 ; training loss : 221769.4715396911\n",
      "for epoch 10 ; training loss : 213060.08634207956\n",
      "for epoch 11 ; training loss : 204322.91705743293\n",
      "for epoch 12 ; training loss : 195562.4971118942\n",
      "for epoch 13 ; training loss : 186786.02673986554\n",
      "for epoch 14 ; training loss : 178002.66054113163\n",
      "for epoch 15 ; training loss : 169229.33589705778\n",
      "for epoch 16 ; training loss : 160487.17259942647\n",
      "for epoch 17 ; training loss : 151802.102282322\n",
      "for epoch 18 ; training loss : 143204.0735417772\n",
      "for epoch 19 ; training loss : 134728.7074726075\n",
      "for epoch 20 ; training loss : 126411.50776910485\n",
      "for epoch 21 ; training loss : 118289.94094575103\n",
      "for epoch 22 ; training loss : 110400.04419316078\n",
      "for epoch 23 ; training loss : 102777.40230141627\n",
      "for epoch 24 ; training loss : 95454.67286534247\n",
      "for epoch 25 ; training loss : 88464.58293177659\n",
      "for epoch 26 ; training loss : 81837.57531099865\n",
      "for epoch 27 ; training loss : 75600.3997122536\n",
      "for epoch 28 ; training loss : 69774.33251090141\n",
      "for epoch 29 ; training loss : 64375.74814770697\n",
      "for epoch 30 ; training loss : 59414.436054290854\n",
      "for epoch 31 ; training loss : 54892.275139098805\n",
      "for epoch 32 ; training loss : 50803.875683081336\n",
      "for epoch 33 ; training loss : 47136.518583444005\n",
      "for epoch 34 ; training loss : 43868.395360861876\n",
      "for epoch 35 ; training loss : 40970.62885386759\n",
      "for epoch 36 ; training loss : 38411.491465887346\n",
      "for epoch 37 ; training loss : 36154.861459570966\n",
      "for epoch 38 ; training loss : 34164.506367935304\n",
      "for epoch 39 ; training loss : 32405.274153298407\n",
      "for epoch 40 ; training loss : 30845.16392477817\n",
      "for epoch 41 ; training loss : 29453.968767352453\n",
      "for epoch 42 ; training loss : 28206.051422246906\n",
      "for epoch 43 ; training loss : 27080.08799106465\n",
      "for epoch 44 ; training loss : 26057.305560873807\n",
      "for epoch 45 ; training loss : 25122.02917187079\n",
      "for epoch 46 ; training loss : 24261.895597853105\n",
      "for epoch 47 ; training loss : 23467.00487195621\n",
      "for epoch 48 ; training loss : 22728.75265448344\n",
      "for epoch 49 ; training loss : 22039.960799788125\n",
      "*** Working on model 500 , iter 4 ***\n",
      "for epoch 0 ; training loss : 295810.49656677246\n",
      "for epoch 1 ; training loss : 287521.7983760834\n",
      "for epoch 2 ; training loss : 279303.5082182884\n",
      "for epoch 3 ; training loss : 271125.9494007826\n",
      "for epoch 4 ; training loss : 262969.053186059\n",
      "for epoch 5 ; training loss : 254808.0554758832\n",
      "for epoch 6 ; training loss : 246619.88147819042\n",
      "for epoch 7 ; training loss : 238385.71803640062\n",
      "for epoch 8 ; training loss : 230093.7076189023\n",
      "for epoch 9 ; training loss : 221741.0381334722\n",
      "for epoch 10 ; training loss : 213328.48726770654\n",
      "for epoch 11 ; training loss : 204857.36130439118\n",
      "for epoch 12 ; training loss : 196332.38959547132\n",
      "for epoch 13 ; training loss : 187763.53720792755\n",
      "for epoch 14 ; training loss : 179163.63744853414\n",
      "for epoch 15 ; training loss : 170550.65377110278\n",
      "for epoch 16 ; training loss : 161945.27751164977\n",
      "for epoch 17 ; training loss : 153378.4028364243\n",
      "for epoch 18 ; training loss : 144881.81460690498\n",
      "for epoch 19 ; training loss : 136489.37233065302\n",
      "for epoch 20 ; training loss : 128235.28019155552\n",
      "for epoch 21 ; training loss : 120155.36496734366\n",
      "for epoch 22 ; training loss : 112286.32086896896\n",
      "for epoch 23 ; training loss : 104664.63721624622\n",
      "for epoch 24 ; training loss : 97324.75320760836\n",
      "for epoch 25 ; training loss : 90301.95751901204\n",
      "for epoch 26 ; training loss : 83626.0457214923\n",
      "for epoch 27 ; training loss : 77325.61273724423\n",
      "for epoch 28 ; training loss : 71423.60782364014\n",
      "for epoch 29 ; training loss : 65939.17470140188\n",
      "for epoch 30 ; training loss : 60885.98985854909\n",
      "for epoch 31 ; training loss : 56267.52667636331\n",
      "for epoch 32 ; training loss : 52081.37258591119\n",
      "for epoch 33 ; training loss : 48316.70410819858\n",
      "for epoch 34 ; training loss : 44953.88823353089\n",
      "for epoch 35 ; training loss : 41966.60705254604\n",
      "for epoch 36 ; training loss : 39322.93960219406\n",
      "for epoch 37 ; training loss : 36987.71363996959\n",
      "for epoch 38 ; training loss : 34926.19985063467\n",
      "for epoch 39 ; training loss : 33102.90122179856\n",
      "for epoch 40 ; training loss : 31484.490838096594\n",
      "for epoch 41 ; training loss : 30041.983457803042\n",
      "for epoch 42 ; training loss : 28749.104361150326\n",
      "for epoch 43 ; training loss : 27583.368671460164\n",
      "for epoch 44 ; training loss : 26525.44933949277\n",
      "for epoch 45 ; training loss : 25559.30826890876\n",
      "for epoch 46 ; training loss : 24671.868744039108\n",
      "for epoch 47 ; training loss : 23852.247911517275\n",
      "for epoch 48 ; training loss : 23091.209719781313\n",
      "for epoch 49 ; training loss : 22381.618888631812\n",
      "*** Working on model 500 , iter 5 ***\n",
      "for epoch 0 ; training loss : 295029.4060153961\n",
      "for epoch 1 ; training loss : 286682.18863487244\n",
      "for epoch 2 ; training loss : 278434.9726905823\n",
      "for epoch 3 ; training loss : 270211.8422050476\n",
      "for epoch 4 ; training loss : 261980.47692394257\n",
      "for epoch 5 ; training loss : 253723.55873939395\n",
      "for epoch 6 ; training loss : 245429.15138590708\n",
      "for epoch 7 ; training loss : 237086.62718078494\n",
      "for epoch 8 ; training loss : 228691.0461179372\n",
      "for epoch 9 ; training loss : 220248.67915943917\n",
      "for epoch 10 ; training loss : 211767.5535036996\n",
      "for epoch 11 ; training loss : 203258.37146599265\n",
      "for epoch 12 ; training loss : 194730.56724587083\n",
      "for epoch 13 ; training loss : 186195.72703228798\n",
      "for epoch 14 ; training loss : 177664.96293846518\n",
      "for epoch 15 ; training loss : 169152.07953012548\n",
      "for epoch 16 ; training loss : 160673.5166760981\n",
      "for epoch 17 ; training loss : 152253.3203993109\n",
      "for epoch 18 ; training loss : 143918.50103138015\n",
      "for epoch 19 ; training loss : 135698.7388222659\n",
      "for epoch 20 ; training loss : 127624.26564341632\n",
      "for epoch 21 ; training loss : 119729.3470343221\n",
      "for epoch 22 ; training loss : 112044.04895057715\n",
      "for epoch 23 ; training loss : 104600.80603130581\n",
      "for epoch 24 ; training loss : 97432.78368303506\n",
      "for epoch 25 ; training loss : 90570.04335387895\n",
      "for epoch 26 ; training loss : 84038.23143965483\n",
      "for epoch 27 ; training loss : 77862.66437810613\n",
      "for epoch 28 ; training loss : 72066.84730928212\n",
      "for epoch 29 ; training loss : 66668.78588843378\n",
      "for epoch 30 ; training loss : 61680.07263139775\n",
      "for epoch 31 ; training loss : 57106.59732333594\n",
      "for epoch 32 ; training loss : 52946.39808686846\n",
      "for epoch 33 ; training loss : 49189.53652454768\n",
      "for epoch 34 ; training loss : 45818.98828023556\n",
      "for epoch 35 ; training loss : 42811.03873954581\n",
      "for epoch 36 ; training loss : 40138.10161014777\n",
      "for epoch 37 ; training loss : 37767.52689835988\n",
      "for epoch 38 ; training loss : 35666.94271376263\n",
      "for epoch 39 ; training loss : 33803.82562393666\n",
      "for epoch 40 ; training loss : 32146.672140773153\n",
      "for epoch 41 ; training loss : 30666.15383235173\n",
      "for epoch 42 ; training loss : 29337.376322642438\n",
      "for epoch 43 ; training loss : 28138.375360157355\n",
      "for epoch 44 ; training loss : 27050.09388487327\n",
      "for epoch 45 ; training loss : 26056.386968247723\n",
      "for epoch 46 ; training loss : 25143.834871859\n",
      "for epoch 47 ; training loss : 24301.40808897183\n",
      "for epoch 48 ; training loss : 23519.687337339856\n",
      "for epoch 49 ; training loss : 22791.34297028218\n",
      "*** Working on model 500 , iter 6 ***\n",
      "for epoch 0 ; training loss : 298085.26822662354\n",
      "for epoch 1 ; training loss : 289610.7696247101\n",
      "for epoch 2 ; training loss : 281189.0939998627\n",
      "for epoch 3 ; training loss : 272768.9553747177\n",
      "for epoch 4 ; training loss : 264324.52467012405\n",
      "for epoch 5 ; training loss : 255827.95202207565\n",
      "for epoch 6 ; training loss : 247258.8066200018\n",
      "for epoch 7 ; training loss : 238605.36489406228\n",
      "for epoch 8 ; training loss : 229863.81848284602\n",
      "for epoch 9 ; training loss : 221035.6240362525\n",
      "for epoch 10 ; training loss : 212128.651614476\n",
      "for epoch 11 ; training loss : 203157.46249019355\n",
      "for epoch 12 ; training loss : 194137.13918170985\n",
      "for epoch 13 ; training loss : 185084.2184177339\n",
      "for epoch 14 ; training loss : 176016.92479899898\n",
      "for epoch 15 ; training loss : 166957.94119664282\n",
      "for epoch 16 ; training loss : 157936.30815982446\n",
      "for epoch 17 ; training loss : 148982.21835167543\n",
      "for epoch 18 ; training loss : 140128.7541095534\n",
      "for epoch 19 ; training loss : 131413.3450983723\n",
      "for epoch 20 ; training loss : 122876.03488086979\n",
      "for epoch 21 ; training loss : 114557.27119792043\n",
      "for epoch 22 ; training loss : 106496.91479803622\n",
      "for epoch 23 ; training loss : 98733.3562021451\n",
      "for epoch 24 ; training loss : 91304.26023675594\n",
      "for epoch 25 ; training loss : 84242.3087424445\n",
      "for epoch 26 ; training loss : 77580.12157142861\n",
      "for epoch 27 ; training loss : 71345.69547413336\n",
      "for epoch 28 ; training loss : 65560.15657076612\n",
      "for epoch 29 ; training loss : 60235.2944122107\n",
      "for epoch 30 ; training loss : 55377.88709766185\n",
      "for epoch 31 ; training loss : 50984.75892415398\n",
      "for epoch 32 ; training loss : 47043.492633928545\n",
      "for epoch 33 ; training loss : 43534.14018145023\n",
      "for epoch 34 ; training loss : 40427.586064185016\n",
      "for epoch 35 ; training loss : 37689.49474422367\n",
      "for epoch 36 ; training loss : 35282.2681067759\n",
      "for epoch 37 ; training loss : 33166.85396527934\n",
      "for epoch 38 ; training loss : 31304.898436715448\n",
      "for epoch 39 ; training loss : 29661.48963105539\n",
      "for epoch 40 ; training loss : 28204.991764922495\n",
      "for epoch 41 ; training loss : 26907.41497086943\n",
      "for epoch 42 ; training loss : 25744.616914395432\n",
      "for epoch 43 ; training loss : 24696.63620034131\n",
      "for epoch 44 ; training loss : 23746.547954012734\n",
      "for epoch 45 ; training loss : 22880.45508462278\n",
      "for epoch 46 ; training loss : 22086.84677543518\n",
      "for epoch 47 ; training loss : 21356.653491270583\n",
      "for epoch 48 ; training loss : 20682.006647463484\n",
      "for epoch 49 ; training loss : 20056.697392353715\n",
      "*** Working on model 500 , iter 7 ***\n",
      "for epoch 0 ; training loss : 294890.19049453735\n",
      "for epoch 1 ; training loss : 286372.8708343506\n",
      "for epoch 2 ; training loss : 277908.39546585083\n",
      "for epoch 3 ; training loss : 269452.2230529785\n",
      "for epoch 4 ; training loss : 260982.88912200928\n",
      "for epoch 5 ; training loss : 252480.72655844688\n",
      "for epoch 6 ; training loss : 243930.1677751541\n",
      "for epoch 7 ; training loss : 235318.2069258392\n",
      "for epoch 8 ; training loss : 226634.27507891878\n",
      "for epoch 9 ; training loss : 217875.14117622375\n",
      "for epoch 10 ; training loss : 209044.60343923792\n",
      "for epoch 11 ; training loss : 200152.18243748238\n",
      "for epoch 12 ; training loss : 191212.74947125465\n",
      "for epoch 13 ; training loss : 182245.51637475193\n",
      "for epoch 14 ; training loss : 173273.02182232984\n",
      "for epoch 15 ; training loss : 164317.57724894956\n",
      "for epoch 16 ; training loss : 155404.66418631515\n",
      "for epoch 17 ; training loss : 146564.36296129413\n",
      "for epoch 18 ; training loss : 137827.16728419624\n",
      "for epoch 19 ; training loss : 129224.95088416107\n",
      "for epoch 20 ; training loss : 120792.58225896955\n",
      "for epoch 21 ; training loss : 112568.37625636905\n",
      "for epoch 22 ; training loss : 104589.61713474011\n",
      "for epoch 23 ; training loss : 96892.6044365773\n",
      "for epoch 24 ; training loss : 89516.75398335373\n",
      "for epoch 25 ; training loss : 82497.55635643564\n",
      "for epoch 26 ; training loss : 75866.02743728831\n",
      "for epoch 27 ; training loss : 69650.07901867929\n",
      "for epoch 28 ; training loss : 63874.07771402225\n",
      "for epoch 29 ; training loss : 58551.58289283264\n",
      "for epoch 30 ; training loss : 53695.07966744624\n",
      "for epoch 31 ; training loss : 49306.870332252685\n",
      "for epoch 32 ; training loss : 45379.02930924267\n",
      "for epoch 33 ; training loss : 41895.260815123445\n",
      "for epoch 34 ; training loss : 38831.235195534355\n",
      "for epoch 35 ; training loss : 36154.63950438454\n",
      "for epoch 36 ; training loss : 33827.347964524175\n",
      "for epoch 37 ; training loss : 31808.903354162234\n",
      "for epoch 38 ; training loss : 30057.946435527527\n",
      "for epoch 39 ; training loss : 28534.961644169947\n",
      "for epoch 40 ; training loss : 27203.423394627847\n",
      "for epoch 41 ; training loss : 26031.115955683694\n",
      "for epoch 42 ; training loss : 24990.895967399265\n",
      "for epoch 43 ; training loss : 24060.206378546067\n",
      "for epoch 44 ; training loss : 23220.26098645328\n",
      "for epoch 45 ; training loss : 22456.39254462876\n",
      "for epoch 46 ; training loss : 21756.38034787137\n",
      "for epoch 47 ; training loss : 21110.53321984796\n",
      "for epoch 48 ; training loss : 20511.621558661755\n",
      "for epoch 49 ; training loss : 19953.615003417304\n",
      "*** Working on model 500 , iter 8 ***\n",
      "for epoch 0 ; training loss : 289929.08631801605\n",
      "for epoch 1 ; training loss : 281571.25182819366\n",
      "for epoch 2 ; training loss : 273257.02125120163\n",
      "for epoch 3 ; training loss : 264945.2771525383\n",
      "for epoch 4 ; training loss : 256620.48668071628\n",
      "for epoch 5 ; training loss : 248271.03978843626\n",
      "for epoch 6 ; training loss : 239884.59325787425\n",
      "for epoch 7 ; training loss : 231451.98662351072\n",
      "for epoch 8 ; training loss : 222969.10610764707\n",
      "for epoch 9 ; training loss : 214437.44157871604\n",
      "for epoch 10 ; training loss : 205859.2201505006\n",
      "for epoch 11 ; training loss : 197242.44697774202\n",
      "for epoch 12 ; training loss : 188598.78670194605\n",
      "for epoch 13 ; training loss : 179940.90285496134\n",
      "for epoch 14 ; training loss : 171283.45268951356\n",
      "for epoch 15 ; training loss : 162645.1564910757\n",
      "for epoch 16 ; training loss : 154048.26180414535\n",
      "for epoch 17 ; training loss : 145519.94031933672\n",
      "for epoch 18 ; training loss : 137091.71524252463\n",
      "for epoch 19 ; training loss : 128797.01721450879\n",
      "for epoch 20 ; training loss : 120674.7403255254\n",
      "for epoch 21 ; training loss : 112758.80533898325\n",
      "for epoch 22 ; training loss : 105082.68325255625\n",
      "for epoch 23 ; training loss : 97683.56688008364\n",
      "for epoch 24 ; training loss : 90594.52933944203\n",
      "for epoch 25 ; training loss : 83847.10549509036\n",
      "for epoch 26 ; training loss : 77468.56381591433\n",
      "for epoch 27 ; training loss : 71484.7254612018\n",
      "for epoch 28 ; training loss : 65913.49494656827\n",
      "for epoch 29 ; training loss : 60769.47071848728\n",
      "for epoch 30 ; training loss : 56059.02377386717\n",
      "for epoch 31 ; training loss : 51780.942122142296\n",
      "for epoch 32 ; training loss : 47925.4791133384\n",
      "for epoch 33 ; training loss : 44476.26201666228\n",
      "for epoch 34 ; training loss : 41408.401136879635\n",
      "for epoch 35 ; training loss : 38691.0983061773\n",
      "for epoch 36 ; training loss : 36290.68291591847\n",
      "for epoch 37 ; training loss : 34172.19561251468\n",
      "for epoch 38 ; training loss : 32299.63270079544\n",
      "for epoch 39 ; training loss : 30640.059415638694\n",
      "for epoch 40 ; training loss : 29163.40817795531\n",
      "for epoch 41 ; training loss : 27842.236719520388\n",
      "for epoch 42 ; training loss : 26652.989900546396\n",
      "for epoch 43 ; training loss : 25575.875876681414\n",
      "for epoch 44 ; training loss : 24594.142796249944\n",
      "for epoch 45 ; training loss : 23693.147911906475\n",
      "for epoch 46 ; training loss : 22861.7409451392\n",
      "for epoch 47 ; training loss : 22091.34044442611\n",
      "for epoch 48 ; training loss : 21374.198888211336\n",
      "for epoch 49 ; training loss : 20704.557448898035\n",
      "*** Working on model 500 , iter 9 ***\n",
      "for epoch 0 ; training loss : 296868.2091560364\n",
      "for epoch 1 ; training loss : 288420.7804927826\n",
      "for epoch 2 ; training loss : 280046.94522333145\n",
      "for epoch 3 ; training loss : 271705.6633088589\n",
      "for epoch 4 ; training loss : 263374.01697677374\n",
      "for epoch 5 ; training loss : 255031.2956026569\n",
      "for epoch 6 ; training loss : 246663.83997428417\n",
      "for epoch 7 ; training loss : 238261.7031354457\n",
      "for epoch 8 ; training loss : 229818.85647446406\n",
      "for epoch 9 ; training loss : 221329.8259652257\n",
      "for epoch 10 ; training loss : 212790.74451865628\n",
      "for epoch 11 ; training loss : 204200.78304976597\n",
      "for epoch 12 ; training loss : 195562.36575084925\n",
      "for epoch 13 ; training loss : 186878.0640104059\n",
      "for epoch 14 ; training loss : 178158.99613099825\n",
      "for epoch 15 ; training loss : 169426.5932139065\n",
      "for epoch 16 ; training loss : 160702.77474945632\n",
      "for epoch 17 ; training loss : 152014.63414564356\n",
      "for epoch 18 ; training loss : 143389.97497953975\n",
      "for epoch 19 ; training loss : 134862.97685823776\n",
      "for epoch 20 ; training loss : 126469.11146412931\n",
      "for epoch 21 ; training loss : 118245.35258978233\n",
      "for epoch 22 ; training loss : 110231.29347207211\n",
      "for epoch 23 ; training loss : 102465.45994680189\n",
      "for epoch 24 ; training loss : 94983.63610162772\n",
      "for epoch 25 ; training loss : 87821.66509423684\n",
      "for epoch 26 ; training loss : 81014.80848219193\n",
      "for epoch 27 ; training loss : 74591.65309052356\n",
      "for epoch 28 ; training loss : 68579.03594265302\n",
      "for epoch 29 ; training loss : 62998.14457150805\n",
      "for epoch 30 ; training loss : 57864.44082080433\n",
      "for epoch 31 ; training loss : 53186.557847658405\n",
      "for epoch 32 ; training loss : 48964.35370885679\n",
      "for epoch 33 ; training loss : 45185.79812901188\n",
      "for epoch 34 ; training loss : 41831.315606971126\n",
      "for epoch 35 ; training loss : 38874.20409396105\n",
      "for epoch 36 ; training loss : 36279.929304914265\n",
      "for epoch 37 ; training loss : 34010.30241078169\n",
      "for epoch 38 ; training loss : 32025.341738189687\n",
      "for epoch 39 ; training loss : 30286.619652985944\n",
      "for epoch 40 ; training loss : 28758.481405992356\n",
      "for epoch 41 ; training loss : 27408.460335922427\n",
      "for epoch 42 ; training loss : 26208.02798808925\n",
      "for epoch 43 ; training loss : 25133.345590886573\n",
      "for epoch 44 ; training loss : 24164.521920767264\n",
      "for epoch 45 ; training loss : 23285.119289809896\n",
      "for epoch 46 ; training loss : 22482.13612103611\n",
      "for epoch 47 ; training loss : 21744.894621016123\n",
      "for epoch 48 ; training loss : 21064.452101283954\n",
      "for epoch 49 ; training loss : 20433.833187815384\n",
      "*** Working on model 1000 , iter 0 ***\n",
      "for epoch 0 ; training loss : 287428.21750354767\n",
      "for epoch 1 ; training loss : 271938.2503786087\n",
      "for epoch 2 ; training loss : 256839.4798247218\n",
      "for epoch 3 ; training loss : 242045.4981071353\n",
      "for epoch 4 ; training loss : 227524.72912245695\n",
      "for epoch 5 ; training loss : 213256.54915490746\n",
      "for epoch 6 ; training loss : 199232.6202840466\n",
      "for epoch 7 ; training loss : 185460.6788534522\n",
      "for epoch 8 ; training loss : 171965.13619255534\n",
      "for epoch 9 ; training loss : 158783.668197806\n",
      "for epoch 10 ; training loss : 145965.28968749475\n",
      "for epoch 11 ; training loss : 133566.79241200443\n",
      "for epoch 12 ; training loss : 121652.16827675872\n",
      "for epoch 13 ; training loss : 110290.82304098457\n",
      "for epoch 14 ; training loss : 99559.17070965283\n",
      "for epoch 15 ; training loss : 89530.5409973003\n",
      "for epoch 16 ; training loss : 80269.6802795399\n",
      "for epoch 17 ; training loss : 71830.82710875987\n",
      "for epoch 18 ; training loss : 64250.64753057202\n",
      "for epoch 19 ; training loss : 57542.72903978411\n",
      "for epoch 20 ; training loss : 51693.04911108781\n",
      "for epoch 21 ; training loss : 46660.78435644647\n",
      "for epoch 22 ; training loss : 42380.49525072641\n",
      "for epoch 23 ; training loss : 38768.29582724831\n",
      "for epoch 24 ; training loss : 35729.860874195525\n",
      "for epoch 25 ; training loss : 33170.36244322735\n",
      "for epoch 26 ; training loss : 31000.752285973693\n",
      "for epoch 27 ; training loss : 29144.44241237617\n",
      "for epoch 28 ; training loss : 27537.7094385542\n",
      "for epoch 29 ; training loss : 26129.228497436052\n",
      "for epoch 30 ; training loss : 24879.343009940945\n",
      "for epoch 31 ; training loss : 23758.217653923697\n",
      "for epoch 32 ; training loss : 22742.962178348855\n",
      "for epoch 33 ; training loss : 21816.26233643914\n",
      "for epoch 34 ; training loss : 20965.372262517718\n",
      "for epoch 35 ; training loss : 20180.473676212343\n",
      "for epoch 36 ; training loss : 19453.66428781912\n",
      "for epoch 37 ; training loss : 18779.46667034758\n",
      "for epoch 38 ; training loss : 18153.128929158556\n",
      "for epoch 39 ; training loss : 17570.39089243338\n",
      "for epoch 40 ; training loss : 17027.849689096707\n",
      "for epoch 41 ; training loss : 16522.335805055805\n",
      "for epoch 42 ; training loss : 16051.188395022182\n",
      "for epoch 43 ; training loss : 15611.89399019863\n",
      "for epoch 44 ; training loss : 15202.488824351167\n",
      "for epoch 45 ; training loss : 14821.426258958643\n",
      "for epoch 46 ; training loss : 14466.673675243106\n",
      "for epoch 47 ; training loss : 14136.175575338595\n",
      "for epoch 48 ; training loss : 13828.1116092156\n",
      "for epoch 49 ; training loss : 13540.826280448426\n",
      "*** Working on model 1000 , iter 1 ***\n",
      "for epoch 0 ; training loss : 283975.50715065\n",
      "for epoch 1 ; training loss : 268877.31136894226\n",
      "for epoch 2 ; training loss : 254172.70407879353\n",
      "for epoch 3 ; training loss : 239742.48659597337\n",
      "for epoch 4 ; training loss : 225541.59441686422\n",
      "for epoch 5 ; training loss : 211550.36977355054\n",
      "for epoch 6 ; training loss : 197770.47949104756\n",
      "for epoch 7 ; training loss : 184219.17577038263\n",
      "for epoch 8 ; training loss : 170928.00283642858\n",
      "for epoch 9 ; training loss : 157938.8744431492\n",
      "for epoch 10 ; training loss : 145301.24054460227\n",
      "for epoch 11 ; training loss : 133077.4348005755\n",
      "for epoch 12 ; training loss : 121331.25394688174\n",
      "for epoch 13 ; training loss : 110128.92789168935\n",
      "for epoch 14 ; training loss : 99541.12186106342\n",
      "for epoch 15 ; training loss : 89636.65547638107\n",
      "for epoch 16 ; training loss : 80478.31868720392\n",
      "for epoch 17 ; training loss : 72118.76474074321\n",
      "for epoch 18 ; training loss : 64591.88662937186\n",
      "for epoch 19 ; training loss : 57911.522761659755\n",
      "for epoch 20 ; training loss : 52067.49695574261\n",
      "for epoch 21 ; training loss : 47025.07128113312\n",
      "for epoch 22 ; training loss : 42724.58586967323\n",
      "for epoch 23 ; training loss : 39088.16433056217\n",
      "for epoch 24 ; training loss : 36025.75953409524\n",
      "for epoch 25 ; training loss : 33445.158513829345\n",
      "for epoch 26 ; training loss : 31259.180477349553\n",
      "for epoch 27 ; training loss : 29390.93035804783\n",
      "for epoch 28 ; training loss : 27775.81269829467\n",
      "for epoch 29 ; training loss : 26362.06261881994\n",
      "for epoch 30 ; training loss : 25109.370690668642\n",
      "for epoch 31 ; training loss : 23986.970564156683\n",
      "for epoch 32 ; training loss : 22971.943842040724\n",
      "for epoch 33 ; training loss : 22046.001711039375\n",
      "for epoch 34 ; training loss : 21195.817287856233\n",
      "for epoch 35 ; training loss : 20411.24471999817\n",
      "for epoch 36 ; training loss : 19684.606480728275\n",
      "for epoch 37 ; training loss : 19010.122534162776\n",
      "for epoch 38 ; training loss : 18382.765021399624\n",
      "for epoch 39 ; training loss : 17798.674163989053\n",
      "for epoch 40 ; training loss : 17254.794593069295\n",
      "for epoch 41 ; training loss : 16748.07185188805\n",
      "for epoch 42 ; training loss : 16275.947658048593\n",
      "for epoch 43 ; training loss : 15835.992479336506\n",
      "for epoch 44 ; training loss : 15425.92345114029\n",
      "for epoch 45 ; training loss : 15043.95077819482\n",
      "for epoch 46 ; training loss : 14688.097735252435\n",
      "for epoch 47 ; training loss : 14356.376850385175\n",
      "for epoch 48 ; training loss : 14047.163287644558\n",
      "for epoch 49 ; training loss : 13758.879605469505\n",
      "*** Working on model 1000 , iter 2 ***\n",
      "for epoch 0 ; training loss : 292565.19373321533\n",
      "for epoch 1 ; training loss : 276917.7576189041\n",
      "for epoch 2 ; training loss : 261719.20317220688\n",
      "for epoch 3 ; training loss : 246871.61847847165\n",
      "for epoch 4 ; training loss : 232322.57944712695\n",
      "for epoch 5 ; training loss : 218037.43526645005\n",
      "for epoch 6 ; training loss : 204002.15040959232\n",
      "for epoch 7 ; training loss : 190218.56830845185\n",
      "for epoch 8 ; training loss : 176704.22883112362\n",
      "for epoch 9 ; training loss : 163491.59440033138\n",
      "for epoch 10 ; training loss : 150623.3594366936\n",
      "for epoch 11 ; training loss : 138150.22160707292\n",
      "for epoch 12 ; training loss : 126129.44434801862\n",
      "for epoch 13 ; training loss : 114630.29478921379\n",
      "for epoch 14 ; training loss : 103725.03261057474\n",
      "for epoch 15 ; training loss : 93489.87990803723\n",
      "for epoch 16 ; training loss : 83989.65315119643\n",
      "for epoch 17 ; training loss : 75281.73509292374\n",
      "for epoch 18 ; training loss : 67409.91648997396\n",
      "for epoch 19 ; training loss : 60398.73573253068\n",
      "for epoch 20 ; training loss : 54248.11500854569\n",
      "for epoch 21 ; training loss : 48929.67648160481\n",
      "for epoch 22 ; training loss : 44388.56690797329\n",
      "for epoch 23 ; training loss : 40547.89088792133\n",
      "for epoch 24 ; training loss : 37314.98125138406\n",
      "for epoch 25 ; training loss : 34593.23188672427\n",
      "for epoch 26 ; training loss : 32290.39454286365\n",
      "for epoch 27 ; training loss : 30323.965434453712\n",
      "for epoch 28 ; training loss : 28624.368621391404\n",
      "for epoch 29 ; training loss : 27135.501828351193\n",
      "for epoch 30 ; training loss : 25814.405391774504\n",
      "for epoch 31 ; training loss : 24628.126770238043\n",
      "for epoch 32 ; training loss : 23552.112221563613\n",
      "for epoch 33 ; training loss : 22567.949494479166\n",
      "for epoch 34 ; training loss : 21661.757343236022\n",
      "for epoch 35 ; training loss : 20823.089230837533\n",
      "for epoch 36 ; training loss : 20044.351604480296\n",
      "for epoch 37 ; training loss : 19319.74472891842\n",
      "for epoch 38 ; training loss : 18644.21476116832\n",
      "for epoch 39 ; training loss : 18013.651087176804\n",
      "for epoch 40 ; training loss : 17425.241028293123\n",
      "for epoch 41 ; training loss : 16876.45695396456\n",
      "for epoch 42 ; training loss : 16364.509609227069\n",
      "for epoch 43 ; training loss : 15886.983053761884\n",
      "for epoch 44 ; training loss : 15441.867130370956\n",
      "for epoch 45 ; training loss : 15027.261569204333\n",
      "for epoch 46 ; training loss : 14641.421542041237\n",
      "for epoch 47 ; training loss : 14282.295829243056\n",
      "for epoch 48 ; training loss : 13947.858690153356\n",
      "for epoch 49 ; training loss : 13636.52817443226\n",
      "*** Working on model 1000 , iter 3 ***\n",
      "for epoch 0 ; training loss : 289373.49292850494\n",
      "for epoch 1 ; training loss : 273716.37203884125\n",
      "for epoch 2 ; training loss : 258445.53484642506\n",
      "for epoch 3 ; training loss : 243466.00965830032\n",
      "for epoch 4 ; training loss : 228729.72263415158\n",
      "for epoch 5 ; training loss : 214203.79979497194\n",
      "for epoch 6 ; training loss : 199880.00673203124\n",
      "for epoch 7 ; training loss : 185775.42167085782\n",
      "for epoch 8 ; training loss : 171925.84415827924\n",
      "for epoch 9 ; training loss : 158379.60033086955\n",
      "for epoch 10 ; training loss : 145190.1888017878\n",
      "for epoch 11 ; training loss : 132421.40928790206\n",
      "for epoch 12 ; training loss : 120144.10562262917\n",
      "for epoch 13 ; training loss : 108436.79752201168\n",
      "for epoch 14 ; training loss : 97382.55439353426\n",
      "for epoch 15 ; training loss : 87064.64387400821\n",
      "for epoch 16 ; training loss : 77555.5031370055\n",
      "for epoch 17 ; training loss : 68913.36660222325\n",
      "for epoch 18 ; training loss : 61175.77354982594\n",
      "for epoch 19 ; training loss : 54356.91212729071\n",
      "for epoch 20 ; training loss : 48442.88429602096\n",
      "for epoch 21 ; training loss : 43389.22424406448\n",
      "for epoch 22 ; training loss : 39125.12732743181\n",
      "for epoch 23 ; training loss : 35560.97888894212\n",
      "for epoch 24 ; training loss : 32594.0191533676\n",
      "for epoch 25 ; training loss : 30121.29031288391\n",
      "for epoch 26 ; training loss : 28047.794147185516\n",
      "for epoch 27 ; training loss : 26292.156147138685\n",
      "for epoch 28 ; training loss : 24786.836773188566\n",
      "for epoch 29 ; training loss : 23478.320793909792\n",
      "for epoch 30 ; training loss : 22326.227891339804\n",
      "for epoch 31 ; training loss : 21300.275204427046\n",
      "for epoch 32 ; training loss : 20377.873568022973\n",
      "for epoch 33 ; training loss : 19542.13350888837\n",
      "for epoch 34 ; training loss : 18780.15319743485\n",
      "for epoch 35 ; training loss : 18082.24680341175\n",
      "for epoch 36 ; training loss : 17440.82743939676\n",
      "for epoch 37 ; training loss : 16849.94856660522\n",
      "for epoch 38 ; training loss : 16304.905584164837\n",
      "for epoch 39 ; training loss : 15801.773556339584\n",
      "for epoch 40 ; training loss : 15336.988801419444\n",
      "for epoch 41 ; training loss : 14907.47427330769\n",
      "for epoch 42 ; training loss : 14510.391147100832\n",
      "for epoch 43 ; training loss : 14143.482092403574\n",
      "for epoch 44 ; training loss : 13804.307271854981\n",
      "for epoch 45 ; training loss : 13490.68532191162\n",
      "for epoch 46 ; training loss : 13200.679000851698\n",
      "for epoch 47 ; training loss : 12932.344680237991\n",
      "for epoch 48 ; training loss : 12683.893185738532\n",
      "for epoch 49 ; training loss : 12453.700117713324\n",
      "*** Working on model 1000 , iter 4 ***\n",
      "for epoch 0 ; training loss : 288352.3530263901\n",
      "for epoch 1 ; training loss : 272669.90178608894\n",
      "for epoch 2 ; training loss : 257481.47436325997\n",
      "for epoch 3 ; training loss : 242655.50668147206\n",
      "for epoch 4 ; training loss : 228138.73775869608\n",
      "for epoch 5 ; training loss : 213896.61878415383\n",
      "for epoch 6 ; training loss : 199914.3401824385\n",
      "for epoch 7 ; training loss : 186193.80241794907\n",
      "for epoch 8 ; training loss : 172749.0470179848\n",
      "for epoch 9 ; training loss : 159608.30527097173\n",
      "for epoch 10 ; training loss : 146810.71547875018\n",
      "for epoch 11 ; training loss : 134406.786375392\n",
      "for epoch 12 ; training loss : 122458.91645894293\n",
      "for epoch 13 ; training loss : 111038.71602189449\n",
      "for epoch 14 ; training loss : 100224.41406752449\n",
      "for epoch 15 ; training loss : 90092.18866732786\n",
      "for epoch 16 ; training loss : 80711.53711605715\n",
      "for epoch 17 ; training loss : 72141.99027640084\n",
      "for epoch 18 ; training loss : 64423.48320468726\n",
      "for epoch 19 ; training loss : 57575.70127911956\n",
      "for epoch 20 ; training loss : 51594.634521460044\n",
      "for epoch 21 ; training loss : 46448.84625668684\n",
      "for epoch 22 ; training loss : 42080.23060337371\n",
      "for epoch 23 ; training loss : 38408.219881607\n",
      "for epoch 24 ; training loss : 35339.072650427\n",
      "for epoch 25 ; training loss : 32774.706014895986\n",
      "for epoch 26 ; training loss : 30620.954739476263\n",
      "for epoch 27 ; training loss : 28795.004057292746\n",
      "for epoch 28 ; training loss : 27227.94904035921\n",
      "for epoch 29 ; training loss : 25863.824246131233\n",
      "for epoch 30 ; training loss : 24659.45150327684\n",
      "for epoch 31 ; training loss : 23582.046103880886\n",
      "for epoch 32 ; training loss : 22607.342931002986\n",
      "for epoch 33 ; training loss : 21717.736792172233\n",
      "for epoch 34 ; training loss : 20899.73774984351\n",
      "for epoch 35 ; training loss : 20143.518219557125\n",
      "for epoch 36 ; training loss : 19441.598438298097\n",
      "for epoch 37 ; training loss : 18788.150731110974\n",
      "for epoch 38 ; training loss : 18178.364474290578\n",
      "for epoch 39 ; training loss : 17608.349699374987\n",
      "for epoch 40 ; training loss : 17075.320192632065\n",
      "for epoch 41 ; training loss : 16576.817154656255\n",
      "for epoch 42 ; training loss : 16110.502063010586\n",
      "for epoch 43 ; training loss : 15674.262992119067\n",
      "for epoch 44 ; training loss : 15266.00344828019\n",
      "for epoch 45 ; training loss : 14884.134502663266\n",
      "for epoch 46 ; training loss : 14527.012748172347\n",
      "for epoch 47 ; training loss : 14192.973710267634\n",
      "for epoch 48 ; training loss : 13880.457119108298\n",
      "for epoch 49 ; training loss : 13588.014418831695\n",
      "*** Working on model 1000 , iter 5 ***\n",
      "for epoch 0 ; training loss : 286542.2048034668\n",
      "for epoch 1 ; training loss : 271918.32518196106\n",
      "for epoch 2 ; training loss : 257626.1919068992\n",
      "for epoch 3 ; training loss : 243567.69533973932\n",
      "for epoch 4 ; training loss : 229701.6404539831\n",
      "for epoch 5 ; training loss : 216007.43423841894\n",
      "for epoch 6 ; training loss : 202480.69596108794\n",
      "for epoch 7 ; training loss : 189128.2348753363\n",
      "for epoch 8 ; training loss : 175977.08426095673\n",
      "for epoch 9 ; training loss : 163073.0148977642\n",
      "for epoch 10 ; training loss : 150466.21960670216\n",
      "for epoch 11 ; training loss : 138217.3093331387\n",
      "for epoch 12 ; training loss : 126396.77270971634\n",
      "for epoch 13 ; training loss : 115078.2102264259\n",
      "for epoch 14 ; training loss : 104337.89858505095\n",
      "for epoch 15 ; training loss : 94248.33060402641\n",
      "for epoch 16 ; training loss : 84873.87759490218\n",
      "for epoch 17 ; training loss : 76272.20837220829\n",
      "for epoch 18 ; training loss : 68481.30190178147\n",
      "for epoch 19 ; training loss : 61522.009500746266\n",
      "for epoch 20 ; training loss : 55392.00145001014\n",
      "for epoch 21 ; training loss : 50062.24978159444\n",
      "for epoch 22 ; training loss : 45479.18321873527\n",
      "for epoch 23 ; training loss : 41568.90169645657\n",
      "for epoch 24 ; training loss : 38245.30049981564\n",
      "for epoch 25 ; training loss : 35420.241739840014\n",
      "for epoch 26 ; training loss : 33008.28480485171\n",
      "for epoch 27 ; training loss : 30933.207283883825\n",
      "for epoch 28 ; training loss : 29130.63039750367\n",
      "for epoch 29 ; training loss : 27547.946343595864\n",
      "for epoch 30 ; training loss : 26142.478924297873\n",
      "for epoch 31 ; training loss : 24881.530899021076\n",
      "for epoch 32 ; training loss : 23740.780998274775\n",
      "for epoch 33 ; training loss : 22700.736230166196\n",
      "for epoch 34 ; training loss : 21746.636575350105\n",
      "for epoch 35 ; training loss : 20867.18407017354\n",
      "for epoch 36 ; training loss : 20053.805276875384\n",
      "for epoch 37 ; training loss : 19299.914905585174\n",
      "for epoch 38 ; training loss : 18599.905177895915\n",
      "for epoch 39 ; training loss : 17948.99302739417\n",
      "for epoch 40 ; training loss : 17343.227551057003\n",
      "for epoch 41 ; training loss : 16779.73707640091\n",
      "for epoch 42 ; training loss : 16255.62046769158\n",
      "for epoch 43 ; training loss : 15768.250922458596\n",
      "for epoch 44 ; training loss : 15315.039968402853\n",
      "for epoch 45 ; training loss : 14894.064716228695\n",
      "for epoch 46 ; training loss : 14502.979265164176\n",
      "for epoch 47 ; training loss : 14139.809393525215\n",
      "for epoch 48 ; training loss : 13802.532929353387\n",
      "for epoch 49 ; training loss : 13489.286250774752\n",
      "*** Working on model 1000 , iter 6 ***\n",
      "for epoch 0 ; training loss : 287141.4386482239\n",
      "for epoch 1 ; training loss : 271406.4731955528\n",
      "for epoch 2 ; training loss : 256048.77003121376\n",
      "for epoch 3 ; training loss : 240983.91010197252\n",
      "for epoch 4 ; training loss : 226175.4822369963\n",
      "for epoch 5 ; training loss : 211597.4821631778\n",
      "for epoch 6 ; training loss : 197245.37974462658\n",
      "for epoch 7 ; training loss : 183139.88696494699\n",
      "for epoch 8 ; training loss : 169315.40230164694\n",
      "for epoch 9 ; training loss : 155823.20548626408\n",
      "for epoch 10 ; training loss : 142723.80389989447\n",
      "for epoch 11 ; training loss : 130088.6525140889\n",
      "for epoch 12 ; training loss : 117994.67062851886\n",
      "for epoch 13 ; training loss : 106519.15024912357\n",
      "for epoch 14 ; training loss : 95739.92400462274\n",
      "for epoch 15 ; training loss : 85729.50583949868\n",
      "for epoch 16 ; training loss : 76552.75567989983\n",
      "for epoch 17 ; training loss : 68257.9111319716\n",
      "for epoch 18 ; training loss : 60872.40565319103\n",
      "for epoch 19 ; training loss : 54397.44259264693\n",
      "for epoch 20 ; training loss : 48807.26529185049\n",
      "for epoch 21 ; training loss : 44047.767086120395\n",
      "for epoch 22 ; training loss : 40039.23988310214\n",
      "for epoch 23 ; training loss : 36685.80422329786\n",
      "for epoch 24 ; training loss : 33884.46386758852\n",
      "for epoch 25 ; training loss : 31535.501044346252\n",
      "for epoch 26 ; training loss : 29549.754067097707\n",
      "for epoch 27 ; training loss : 27851.845111793118\n",
      "for epoch 28 ; training loss : 26380.887455790187\n",
      "for epoch 29 ; training loss : 25089.583226222574\n",
      "for epoch 30 ; training loss : 23941.95994697139\n",
      "for epoch 31 ; training loss : 22911.014946905794\n",
      "for epoch 32 ; training loss : 21976.52916737197\n",
      "for epoch 33 ; training loss : 21122.964397784206\n",
      "for epoch 34 ; training loss : 20338.80730257681\n",
      "for epoch 35 ; training loss : 19615.297435825705\n",
      "for epoch 36 ; training loss : 18945.675929242643\n",
      "for epoch 37 ; training loss : 18324.484978248336\n",
      "for epoch 38 ; training loss : 17747.157892260293\n",
      "for epoch 39 ; training loss : 17209.939842855572\n",
      "for epoch 40 ; training loss : 16709.58749098587\n",
      "for epoch 41 ; training loss : 16243.194578318304\n",
      "for epoch 42 ; training loss : 15808.473794495105\n",
      "for epoch 43 ; training loss : 15403.273877518091\n",
      "for epoch 44 ; training loss : 15025.316429118539\n",
      "for epoch 45 ; training loss : 14672.417079466803\n",
      "for epoch 46 ; training loss : 14342.880919369301\n",
      "for epoch 47 ; training loss : 14035.295597825781\n",
      "for epoch 48 ; training loss : 13748.216603177745\n",
      "for epoch 49 ; training loss : 13479.830395405312\n",
      "*** Working on model 1000 , iter 7 ***\n",
      "for epoch 0 ; training loss : 292078.06946086884\n",
      "for epoch 1 ; training loss : 276930.36532354355\n",
      "for epoch 2 ; training loss : 262172.5876634717\n",
      "for epoch 3 ; training loss : 247726.89305323362\n",
      "for epoch 4 ; training loss : 233545.19524059445\n",
      "for epoch 5 ; training loss : 219584.49637051777\n",
      "for epoch 6 ; training loss : 205823.74791403115\n",
      "for epoch 7 ; training loss : 192262.8693607638\n",
      "for epoch 8 ; training loss : 178919.37851787917\n",
      "for epoch 9 ; training loss : 165821.6222618483\n",
      "for epoch 10 ; training loss : 153010.51001389982\n",
      "for epoch 11 ; training loss : 140537.009871552\n",
      "for epoch 12 ; training loss : 128459.49116727497\n",
      "for epoch 13 ; training loss : 116847.91357718408\n",
      "for epoch 14 ; training loss : 105778.59753049986\n",
      "for epoch 15 ; training loss : 95330.20062181473\n",
      "for epoch 16 ; training loss : 85574.82434209157\n",
      "for epoch 17 ; training loss : 76577.03230734039\n",
      "for epoch 18 ; training loss : 68389.55022498642\n",
      "for epoch 19 ; training loss : 61050.151696082205\n",
      "for epoch 20 ; training loss : 54571.693677193485\n",
      "for epoch 21 ; training loss : 48942.831558528706\n",
      "for epoch 22 ; training loss : 44123.45643079965\n",
      "for epoch 23 ; training loss : 40050.31264160332\n",
      "for epoch 24 ; training loss : 36639.922083555604\n",
      "for epoch 25 ; training loss : 33795.96838511519\n",
      "for epoch 26 ; training loss : 31421.00702927122\n",
      "for epoch 27 ; training loss : 29424.593804654785\n",
      "for epoch 28 ; training loss : 27728.544295904052\n",
      "for epoch 29 ; training loss : 26267.907941036625\n",
      "for epoch 30 ; training loss : 24991.658255503695\n",
      "for epoch 31 ; training loss : 23860.962078452256\n",
      "for epoch 32 ; training loss : 22846.437127330926\n",
      "for epoch 33 ; training loss : 21926.674868679664\n",
      "for epoch 34 ; training loss : 21085.59175208457\n",
      "for epoch 35 ; training loss : 20311.28992317291\n",
      "for epoch 36 ; training loss : 19594.907195281237\n",
      "for epoch 37 ; training loss : 18929.44374423928\n",
      "for epoch 38 ; training loss : 18309.79769892707\n",
      "for epoch 39 ; training loss : 17731.872359547648\n",
      "for epoch 40 ; training loss : 17192.37413414099\n",
      "for epoch 41 ; training loss : 16688.458709198807\n",
      "for epoch 42 ; training loss : 16217.445053777425\n",
      "for epoch 43 ; training loss : 15777.314858831167\n",
      "for epoch 44 ; training loss : 15365.982805284031\n",
      "for epoch 45 ; training loss : 14981.630866604333\n",
      "for epoch 46 ; training loss : 14622.399917446863\n",
      "for epoch 47 ; training loss : 14286.714854534017\n",
      "for epoch 48 ; training loss : 13972.919459568278\n",
      "for epoch 49 ; training loss : 13679.450959382346\n",
      "*** Working on model 1000 , iter 8 ***\n",
      "for epoch 0 ; training loss : 291279.7340593338\n",
      "for epoch 1 ; training loss : 275767.4964570999\n",
      "for epoch 2 ; training loss : 260655.7349705249\n",
      "for epoch 3 ; training loss : 245865.56654447317\n",
      "for epoch 4 ; training loss : 231352.52470433793\n",
      "for epoch 5 ; training loss : 217080.74512590375\n",
      "for epoch 6 ; training loss : 203036.32488053665\n",
      "for epoch 7 ; training loss : 189222.08179665415\n",
      "for epoch 8 ; training loss : 175659.03289359715\n",
      "for epoch 9 ; training loss : 162387.84465868538\n",
      "for epoch 10 ; training loss : 149459.3444657796\n",
      "for epoch 11 ; training loss : 136932.6446557436\n",
      "for epoch 12 ; training loss : 124877.61714253109\n",
      "for epoch 13 ; training loss : 113375.86685750389\n",
      "for epoch 14 ; training loss : 102505.48572420282\n",
      "for epoch 15 ; training loss : 92341.35124500381\n",
      "for epoch 16 ; training loss : 82952.47240593634\n",
      "for epoch 17 ; training loss : 74391.13290087739\n",
      "for epoch 18 ; training loss : 66694.05101789918\n",
      "for epoch 19 ; training loss : 59875.7458671522\n",
      "for epoch 20 ; training loss : 53922.17928125523\n",
      "for epoch 21 ; training loss : 48792.45885194829\n",
      "for epoch 22 ; training loss : 44420.45671490696\n",
      "for epoch 23 ; training loss : 40721.56171400496\n",
      "for epoch 24 ; training loss : 37601.61305698191\n",
      "for epoch 25 ; training loss : 34964.54423012286\n",
      "for epoch 26 ; training loss : 32721.85813763144\n",
      "for epoch 27 ; training loss : 30795.191849308205\n",
      "for epoch 28 ; training loss : 29120.71966237096\n",
      "for epoch 29 ; training loss : 27646.507624113336\n",
      "for epoch 30 ; training loss : 26332.421345049555\n",
      "for epoch 31 ; training loss : 25147.486664789496\n",
      "for epoch 32 ; training loss : 24068.903812062694\n",
      "for epoch 33 ; training loss : 23079.435984005075\n",
      "for epoch 34 ; training loss : 22166.040061029988\n",
      "for epoch 35 ; training loss : 21318.660366479526\n",
      "for epoch 36 ; training loss : 20529.916490415028\n",
      "for epoch 37 ; training loss : 19793.79988016648\n",
      "for epoch 38 ; training loss : 19106.14667029702\n",
      "for epoch 39 ; training loss : 18463.073593665802\n",
      "for epoch 40 ; training loss : 17861.139376932522\n",
      "for epoch 41 ; training loss : 17297.662741617904\n",
      "for epoch 42 ; training loss : 16770.05409925957\n",
      "for epoch 43 ; training loss : 16275.998132067241\n",
      "for epoch 44 ; training loss : 15813.472123036438\n",
      "for epoch 45 ; training loss : 15380.789807705092\n",
      "for epoch 46 ; training loss : 14976.112133803737\n",
      "for epoch 47 ; training loss : 14597.555567331237\n",
      "for epoch 48 ; training loss : 14243.807463541005\n",
      "for epoch 49 ; training loss : 13913.05576130192\n",
      "*** Working on model 1000 , iter 9 ***\n",
      "for epoch 0 ; training loss : 296026.65634536743\n",
      "for epoch 1 ; training loss : 280251.67512369156\n",
      "for epoch 2 ; training loss : 264881.2355418205\n",
      "for epoch 3 ; training loss : 249835.8106490001\n",
      "for epoch 4 ; training loss : 235068.81363158673\n",
      "for epoch 5 ; training loss : 220546.4524754329\n",
      "for epoch 6 ; training loss : 206251.49820040166\n",
      "for epoch 7 ; training loss : 192187.37675394467\n",
      "for epoch 8 ; training loss : 178373.3464798131\n",
      "for epoch 9 ; training loss : 164847.08942395728\n",
      "for epoch 10 ; training loss : 151661.30294771958\n",
      "for epoch 11 ; training loss : 138877.3868947178\n",
      "for epoch 12 ; training loss : 126562.64765227528\n",
      "for epoch 13 ; training loss : 114796.31893909993\n",
      "for epoch 14 ; training loss : 103660.16334593482\n",
      "for epoch 15 ; training loss : 93234.97232046761\n",
      "for epoch 16 ; training loss : 83592.73312976956\n",
      "for epoch 17 ; training loss : 74792.9599343313\n",
      "for epoch 18 ; training loss : 66876.37094558761\n",
      "for epoch 19 ; training loss : 59861.35314828018\n",
      "for epoch 20 ; training loss : 53737.30298944541\n",
      "for epoch 21 ; training loss : 48468.281557714836\n",
      "for epoch 22 ; training loss : 43989.24386217056\n",
      "for epoch 23 ; training loss : 40213.52374134096\n",
      "for epoch 24 ; training loss : 37041.359112755046\n",
      "for epoch 25 ; training loss : 34371.67485615151\n",
      "for epoch 26 ; training loss : 32111.408118857536\n",
      "for epoch 27 ; training loss : 30179.15807606536\n",
      "for epoch 28 ; training loss : 28507.40583647386\n",
      "for epoch 29 ; training loss : 27042.8877034467\n",
      "for epoch 30 ; training loss : 25744.671332205242\n",
      "for epoch 31 ; training loss : 24581.14089273248\n",
      "for epoch 32 ; training loss : 23528.47812232928\n",
      "for epoch 33 ; training loss : 22568.8439213096\n",
      "for epoch 34 ; training loss : 21688.94427485013\n",
      "for epoch 35 ; training loss : 20878.609216073855\n",
      "for epoch 36 ; training loss : 20129.862020708795\n",
      "for epoch 37 ; training loss : 19436.084373065038\n",
      "for epoch 38 ; training loss : 18792.095308301945\n",
      "for epoch 39 ; training loss : 18193.55754514909\n",
      "for epoch 40 ; training loss : 17637.053979048753\n",
      "for epoch 41 ; training loss : 17119.494305578177\n",
      "for epoch 42 ; training loss : 16637.836048726807\n",
      "for epoch 43 ; training loss : 16189.607599523282\n",
      "for epoch 44 ; training loss : 15772.50594745003\n",
      "for epoch 45 ; training loss : 15384.357077632914\n",
      "for epoch 46 ; training loss : 15023.007453385158\n",
      "for epoch 47 ; training loss : 14686.462568458752\n",
      "for epoch 48 ; training loss : 14372.980677641579\n",
      "for epoch 49 ; training loss : 14080.805914966753\n",
      "*** Working on model 5000 , iter 0 ***\n",
      "for epoch 0 ; training loss : 263487.05231663585\n",
      "for epoch 1 ; training loss : 203199.34862182976\n",
      "for epoch 2 ; training loss : 155563.04123436473\n",
      "for epoch 3 ; training loss : 118219.9422506016\n",
      "for epoch 4 ; training loss : 89473.28367462085\n",
      "for epoch 5 ; training loss : 67945.14227032753\n",
      "for epoch 6 ; training loss : 52432.540869564225\n",
      "for epoch 7 ; training loss : 41726.96392399911\n",
      "for epoch 8 ; training loss : 34554.84645909876\n",
      "for epoch 9 ; training loss : 29716.393014602247\n",
      "for epoch 10 ; training loss : 26279.70468194801\n",
      "for epoch 11 ; training loss : 23652.126748487877\n",
      "for epoch 12 ; training loss : 21512.652084578567\n",
      "for epoch 13 ; training loss : 19702.864045040682\n",
      "for epoch 14 ; training loss : 18144.988096517685\n",
      "for epoch 15 ; training loss : 16796.82680167338\n",
      "for epoch 16 ; training loss : 15631.120070384135\n",
      "for epoch 17 ; training loss : 14626.752891081505\n",
      "for epoch 18 ; training loss : 13764.929586922619\n",
      "for epoch 19 ; training loss : 13028.290505741461\n",
      "for epoch 20 ; training loss : 12400.264187484514\n",
      "for epoch 21 ; training loss : 11865.283736341346\n",
      "for epoch 22 ; training loss : 11409.032050025824\n",
      "for epoch 23 ; training loss : 11018.491364491583\n",
      "for epoch 24 ; training loss : 10682.437292171235\n",
      "for epoch 25 ; training loss : 10391.133790586497\n",
      "for epoch 26 ; training loss : 10136.255043592522\n",
      "for epoch 27 ; training loss : 9911.031236545765\n",
      "for epoch 28 ; training loss : 9710.053399789875\n",
      "for epoch 29 ; training loss : 9528.97263866011\n",
      "for epoch 30 ; training loss : 9364.270489383565\n",
      "for epoch 31 ; training loss : 9213.247259649921\n",
      "for epoch 32 ; training loss : 9073.734137837022\n",
      "for epoch 33 ; training loss : 8943.936365832546\n",
      "for epoch 34 ; training loss : 8822.444588990416\n",
      "for epoch 35 ; training loss : 8708.169937549555\n",
      "for epoch 36 ; training loss : 8600.190016983492\n",
      "for epoch 37 ; training loss : 8497.84006302906\n",
      "for epoch 38 ; training loss : 8400.493890548707\n",
      "for epoch 39 ; training loss : 8307.629921466436\n",
      "for epoch 40 ; training loss : 8218.8266498538\n",
      "for epoch 41 ; training loss : 8133.707286032266\n",
      "for epoch 42 ; training loss : 8051.975513388781\n",
      "for epoch 43 ; training loss : 7973.403184671803\n",
      "for epoch 44 ; training loss : 7897.768229648573\n",
      "for epoch 45 ; training loss : 7824.859413127342\n",
      "for epoch 46 ; training loss : 7754.507124091295\n",
      "for epoch 47 ; training loss : 7686.53829723498\n",
      "for epoch 48 ; training loss : 7620.779776434312\n",
      "for epoch 49 ; training loss : 7557.138688290579\n",
      "*** Working on model 5000 , iter 1 ***\n",
      "for epoch 0 ; training loss : 258142.81546765566\n",
      "for epoch 1 ; training loss : 200246.77003444824\n",
      "for epoch 2 ; training loss : 154674.83928012755\n",
      "for epoch 3 ; training loss : 118828.25635881095\n",
      "for epoch 4 ; training loss : 90914.97378034821\n",
      "for epoch 5 ; training loss : 69616.81922328752\n",
      "for epoch 6 ; training loss : 53914.43983803785\n",
      "for epoch 7 ; training loss : 42818.97217364359\n",
      "for epoch 8 ; training loss : 35246.54388231563\n",
      "for epoch 9 ; training loss : 30105.32562887449\n",
      "for epoch 10 ; training loss : 26484.599144295804\n",
      "for epoch 11 ; training loss : 23763.924526213028\n",
      "for epoch 12 ; training loss : 21585.645933250664\n",
      "for epoch 13 ; training loss : 19764.31305656396\n",
      "for epoch 14 ; training loss : 18206.209087425435\n",
      "for epoch 15 ; training loss : 16861.119439719638\n",
      "for epoch 16 ; training loss : 15698.151616490679\n",
      "for epoch 17 ; training loss : 14694.38757346617\n",
      "for epoch 18 ; training loss : 13830.740175205443\n",
      "for epoch 19 ; training loss : 13089.690862436491\n",
      "for epoch 20 ; training loss : 12455.115750422992\n",
      "for epoch 21 ; training loss : 11912.15017212327\n",
      "for epoch 22 ; training loss : 11446.887977333554\n",
      "for epoch 23 ; training loss : 11046.950434349696\n",
      "for epoch 24 ; training loss : 10701.470536875644\n",
      "for epoch 25 ; training loss : 10400.96580449674\n",
      "for epoch 26 ; training loss : 10137.579386810936\n",
      "for epoch 27 ; training loss : 9904.697174523055\n",
      "for epoch 28 ; training loss : 9696.855491754399\n",
      "for epoch 29 ; training loss : 9509.621294986442\n",
      "for epoch 30 ; training loss : 9339.534524646126\n",
      "for epoch 31 ; training loss : 9183.787373114683\n",
      "for epoch 32 ; training loss : 9040.13054517913\n",
      "for epoch 33 ; training loss : 8906.712520151515\n",
      "for epoch 34 ; training loss : 8782.0722194802\n",
      "for epoch 35 ; training loss : 8665.125807756558\n",
      "for epoch 36 ; training loss : 8554.957865739416\n",
      "for epoch 37 ; training loss : 8450.839074457937\n",
      "for epoch 38 ; training loss : 8352.060607836931\n",
      "for epoch 39 ; training loss : 8258.128219487178\n",
      "for epoch 40 ; training loss : 8168.552095680498\n",
      "for epoch 41 ; training loss : 8082.950187690545\n",
      "for epoch 42 ; training loss : 8000.962372675902\n",
      "for epoch 43 ; training loss : 7922.313541464391\n",
      "for epoch 44 ; training loss : 7846.762767221488\n",
      "for epoch 45 ; training loss : 7774.0987601046545\n",
      "for epoch 46 ; training loss : 7704.124075702333\n",
      "for epoch 47 ; training loss : 7636.654041974747\n",
      "for epoch 48 ; training loss : 7571.536116545147\n",
      "for epoch 49 ; training loss : 7508.63152724515\n",
      "*** Working on model 5000 , iter 2 ***\n",
      "for epoch 0 ; training loss : 257639.62729163468\n",
      "for epoch 1 ; training loss : 199055.66134794988\n",
      "for epoch 2 ; training loss : 152942.3931515857\n",
      "for epoch 3 ; training loss : 116876.8306645212\n",
      "for epoch 4 ; training loss : 89096.19909850412\n",
      "for epoch 5 ; training loss : 68221.95319425699\n",
      "for epoch 6 ; training loss : 53101.53810720099\n",
      "for epoch 7 ; training loss : 42593.67885582929\n",
      "for epoch 8 ; training loss : 35496.92035108137\n",
      "for epoch 9 ; training loss : 30673.631914211383\n",
      "for epoch 10 ; training loss : 27230.924734761356\n",
      "for epoch 11 ; training loss : 24591.656122376447\n",
      "for epoch 12 ; training loss : 22436.790808552585\n",
      "for epoch 13 ; training loss : 20605.768843845188\n",
      "for epoch 14 ; training loss : 19019.50103225914\n",
      "for epoch 15 ; training loss : 17635.80475132446\n",
      "for epoch 16 ; training loss : 16428.24363248766\n",
      "for epoch 17 ; training loss : 15376.836138818693\n",
      "for epoch 18 ; training loss : 14464.443442172313\n",
      "for epoch 19 ; training loss : 13675.48551672099\n",
      "for epoch 20 ; training loss : 12995.243712714844\n",
      "for epoch 21 ; training loss : 12409.701260800739\n",
      "for epoch 22 ; training loss : 11905.621741831885\n",
      "for epoch 23 ; training loss : 11470.907490559039\n",
      "for epoch 24 ; training loss : 11094.585861744741\n",
      "for epoch 25 ; training loss : 10767.058192792003\n",
      "for epoch 26 ; training loss : 10479.945132415276\n",
      "for epoch 27 ; training loss : 10226.115138526657\n",
      "for epoch 28 ; training loss : 9999.7886991064\n",
      "for epoch 29 ; training loss : 9796.16004423704\n",
      "for epoch 30 ; training loss : 9611.336707767688\n",
      "for epoch 31 ; training loss : 9442.18411657993\n",
      "for epoch 32 ; training loss : 9286.193940366764\n",
      "for epoch 33 ; training loss : 9141.417652345815\n",
      "for epoch 34 ; training loss : 9006.301666434883\n",
      "for epoch 35 ; training loss : 8879.534089734676\n",
      "for epoch 36 ; training loss : 8760.076185734957\n",
      "for epoch 37 ; training loss : 8647.096841046008\n",
      "for epoch 38 ; training loss : 8539.88343141615\n",
      "for epoch 39 ; training loss : 8437.875302000662\n",
      "for epoch 40 ; training loss : 8340.532959019838\n",
      "for epoch 41 ; training loss : 8247.51132306477\n",
      "for epoch 42 ; training loss : 8158.4166933464185\n",
      "for epoch 43 ; training loss : 8072.997492117302\n",
      "for epoch 44 ; training loss : 7990.943562532197\n",
      "for epoch 45 ; training loss : 7912.071446996037\n",
      "for epoch 46 ; training loss : 7836.167555642958\n",
      "for epoch 47 ; training loss : 7763.084566678328\n",
      "for epoch 48 ; training loss : 7692.604002298369\n",
      "for epoch 49 ; training loss : 7624.570186647736\n",
      "*** Working on model 5000 , iter 3 ***\n",
      "for epoch 0 ; training loss : 262252.4420683682\n",
      "for epoch 1 ; training loss : 203941.23478205747\n",
      "for epoch 2 ; training loss : 157906.93863104656\n",
      "for epoch 3 ; training loss : 121663.72473602928\n",
      "for epoch 4 ; training loss : 93464.25147021575\n",
      "for epoch 5 ; training loss : 71984.44812220009\n",
      "for epoch 6 ; training loss : 56153.37637710737\n",
      "for epoch 7 ; training loss : 44931.645682382165\n",
      "for epoch 8 ; training loss : 37205.65993457359\n",
      "for epoch 9 ; training loss : 31881.09213036001\n",
      "for epoch 10 ; training loss : 28063.451633325312\n",
      "for epoch 11 ; training loss : 25149.92467236008\n",
      "for epoch 12 ; training loss : 22792.45880662756\n",
      "for epoch 13 ; training loss : 20808.914505877212\n",
      "for epoch 14 ; training loss : 19105.41692958068\n",
      "for epoch 15 ; training loss : 17630.91822478341\n",
      "for epoch 16 ; training loss : 16353.316707302176\n",
      "for epoch 17 ; training loss : 15249.058997601795\n",
      "for epoch 18 ; training loss : 14298.185818107744\n",
      "for epoch 19 ; training loss : 13482.535010253428\n",
      "for epoch 20 ; training loss : 12785.027649597061\n",
      "for epoch 21 ; training loss : 12189.550094023522\n",
      "for epoch 22 ; training loss : 11681.22727753401\n",
      "for epoch 23 ; training loss : 11246.443859776748\n",
      "for epoch 24 ; training loss : 10873.043850015601\n",
      "for epoch 25 ; training loss : 10550.444479351328\n",
      "for epoch 26 ; training loss : 10269.563554084918\n",
      "for epoch 27 ; training loss : 10022.864216079473\n",
      "for epoch 28 ; training loss : 9804.133845048797\n",
      "for epoch 29 ; training loss : 9608.33257595198\n",
      "for epoch 30 ; training loss : 9431.398601043988\n",
      "for epoch 31 ; training loss : 9270.09459190106\n",
      "for epoch 32 ; training loss : 9121.921661981454\n",
      "for epoch 33 ; training loss : 8984.854605841363\n",
      "for epoch 34 ; training loss : 8857.26883973532\n",
      "for epoch 35 ; training loss : 8737.812050045002\n",
      "for epoch 36 ; training loss : 8625.439673877147\n",
      "for epoch 37 ; training loss : 8519.239728687277\n",
      "for epoch 38 ; training loss : 8418.55554167238\n",
      "for epoch 39 ; training loss : 8322.774492093493\n",
      "for epoch 40 ; training loss : 8231.44540739304\n",
      "for epoch 41 ; training loss : 8144.102496808748\n",
      "for epoch 42 ; training loss : 8060.389698120012\n",
      "for epoch 43 ; training loss : 7980.057703552622\n",
      "for epoch 44 ; training loss : 7902.847035715211\n",
      "for epoch 45 ; training loss : 7828.527741147947\n",
      "for epoch 46 ; training loss : 7756.936530727777\n",
      "for epoch 47 ; training loss : 7687.849224963356\n",
      "for epoch 48 ; training loss : 7621.090619266295\n",
      "for epoch 49 ; training loss : 7556.5109949600155\n",
      "*** Working on model 5000 , iter 4 ***\n",
      "for epoch 0 ; training loss : 260172.46435832977\n",
      "for epoch 1 ; training loss : 201705.943342762\n",
      "for epoch 2 ; training loss : 155629.18863560073\n",
      "for epoch 3 ; training loss : 119416.81048627757\n",
      "for epoch 4 ; training loss : 91301.40062601876\n",
      "for epoch 5 ; training loss : 69960.31373348447\n",
      "for epoch 6 ; training loss : 54328.377341374886\n",
      "for epoch 7 ; training loss : 43347.84390991321\n",
      "for epoch 8 ; training loss : 35873.53403511306\n",
      "for epoch 9 ; training loss : 30782.406693377532\n",
      "for epoch 10 ; training loss : 27164.95518914465\n",
      "for epoch 11 ; training loss : 24417.21484642823\n",
      "for epoch 12 ; training loss : 22197.80363277813\n",
      "for epoch 13 ; training loss : 20331.115856409888\n",
      "for epoch 14 ; training loss : 18728.147209450137\n",
      "for epoch 15 ; training loss : 17340.42301906668\n",
      "for epoch 16 ; training loss : 16137.964681901532\n",
      "for epoch 17 ; training loss : 15098.123164814198\n",
      "for epoch 18 ; training loss : 14201.896884857444\n",
      "for epoch 19 ; training loss : 13431.9385870416\n",
      "for epoch 20 ; training loss : 12771.9651491708\n",
      "for epoch 21 ; training loss : 12206.73850277823\n",
      "for epoch 22 ; training loss : 11722.197461912456\n",
      "for epoch 23 ; training loss : 11305.69390601119\n",
      "for epoch 24 ; training loss : 10945.945779387606\n",
      "for epoch 25 ; training loss : 10633.092636120477\n",
      "for epoch 26 ; training loss : 10358.777826371224\n",
      "for epoch 27 ; training loss : 10116.147958252346\n",
      "for epoch 28 ; training loss : 9899.56422190035\n",
      "for epoch 29 ; training loss : 9704.346680608145\n",
      "for epoch 30 ; training loss : 9526.858038032897\n",
      "for epoch 31 ; training loss : 9364.148676353507\n",
      "for epoch 32 ; training loss : 9213.929173515648\n",
      "for epoch 33 ; training loss : 9074.317227913096\n",
      "for epoch 34 ; training loss : 8943.833869716313\n",
      "for epoch 35 ; training loss : 8821.252277542662\n",
      "for epoch 36 ; training loss : 8705.640547292656\n",
      "for epoch 37 ; training loss : 8596.221536184894\n",
      "for epoch 38 ; training loss : 8492.28642790491\n",
      "for epoch 39 ; training loss : 8393.351032166222\n",
      "for epoch 40 ; training loss : 8298.929384133691\n",
      "for epoch 41 ; training loss : 8208.63191184729\n",
      "for epoch 42 ; training loss : 8122.082714353226\n",
      "for epoch 43 ; training loss : 8038.969583438462\n",
      "for epoch 44 ; training loss : 7959.032797521868\n",
      "for epoch 45 ; training loss : 7882.062621854304\n",
      "for epoch 46 ; training loss : 7807.875379367804\n",
      "for epoch 47 ; training loss : 7736.279160191196\n",
      "for epoch 48 ; training loss : 7667.073882840341\n",
      "for epoch 49 ; training loss : 7600.165735069575\n",
      "*** Working on model 5000 , iter 5 ***\n",
      "for epoch 0 ; training loss : 259699.48519808054\n",
      "for epoch 1 ; training loss : 200901.41227617604\n",
      "for epoch 2 ; training loss : 154695.74444636935\n",
      "for epoch 3 ; training loss : 118510.30231901072\n",
      "for epoch 4 ; training loss : 90555.8898829706\n",
      "for epoch 5 ; training loss : 69460.83007944352\n",
      "for epoch 6 ; training loss : 54088.919640226755\n",
      "for epoch 7 ; training loss : 43316.36195967486\n",
      "for epoch 8 ; training loss : 35959.342758760875\n",
      "for epoch 9 ; training loss : 30897.768378208057\n",
      "for epoch 10 ; training loss : 27253.243889188194\n",
      "for epoch 11 ; training loss : 24456.015073536666\n",
      "for epoch 12 ; training loss : 22186.44231576208\n",
      "for epoch 13 ; training loss : 20278.487078153674\n",
      "for epoch 14 ; training loss : 18646.302912343308\n",
      "for epoch 15 ; training loss : 17241.383235425634\n",
      "for epoch 16 ; training loss : 16031.951281152898\n",
      "for epoch 17 ; training loss : 14993.673610173108\n",
      "for epoch 18 ; training loss : 14105.455226236605\n",
      "for epoch 19 ; training loss : 13348.014896413428\n",
      "for epoch 20 ; training loss : 12703.49081477875\n",
      "for epoch 21 ; training loss : 12155.16602968931\n",
      "for epoch 22 ; training loss : 11687.882505954476\n",
      "for epoch 23 ; training loss : 11287.931309221749\n",
      "for epoch 24 ; training loss : 10943.390618693025\n",
      "for epoch 25 ; training loss : 10644.12089309495\n",
      "for epoch 26 ; training loss : 10381.691106524755\n",
      "for epoch 27 ; training loss : 10149.1170943715\n",
      "for epoch 28 ; training loss : 9940.853259562165\n",
      "for epoch 29 ; training loss : 9752.46121873759\n",
      "for epoch 30 ; training loss : 9580.482348579797\n",
      "for epoch 31 ; training loss : 9422.155032986171\n",
      "for epoch 32 ; training loss : 9275.27393846371\n",
      "for epoch 33 ; training loss : 9138.148681913102\n",
      "for epoch 34 ; training loss : 9009.452692930252\n",
      "for epoch 35 ; training loss : 8888.09814996837\n",
      "for epoch 36 ; training loss : 8773.248991485649\n",
      "for epoch 37 ; training loss : 8664.171644280817\n",
      "for epoch 38 ; training loss : 8560.309516351801\n",
      "for epoch 39 ; training loss : 8461.132001822421\n",
      "for epoch 40 ; training loss : 8366.252246958902\n",
      "for epoch 41 ; training loss : 8275.31726981634\n",
      "for epoch 42 ; training loss : 8188.027119418883\n",
      "for epoch 43 ; training loss : 8104.093496550766\n",
      "for epoch 44 ; training loss : 8023.317938558539\n",
      "for epoch 45 ; training loss : 7945.458148959035\n",
      "for epoch 46 ; training loss : 7870.357294439687\n",
      "for epoch 47 ; training loss : 7797.862123430008\n",
      "for epoch 48 ; training loss : 7727.829672355729\n",
      "for epoch 49 ; training loss : 7660.087638682548\n",
      "*** Working on model 5000 , iter 6 ***\n",
      "for epoch 0 ; training loss : 261148.0372646749\n",
      "for epoch 1 ; training loss : 202897.65870955586\n",
      "for epoch 2 ; training loss : 157029.30326131347\n",
      "for epoch 3 ; training loss : 121014.4517907342\n",
      "for epoch 4 ; training loss : 93069.59527424636\n",
      "for epoch 5 ; training loss : 71837.33794037254\n",
      "for epoch 6 ; training loss : 56223.017680393605\n",
      "for epoch 7 ; training loss : 45165.18223863805\n",
      "for epoch 8 ; training loss : 37538.28876663203\n",
      "for epoch 9 ; training loss : 32255.606793212602\n",
      "for epoch 10 ; training loss : 28441.43988891099\n",
      "for epoch 11 ; training loss : 25512.892909862974\n",
      "for epoch 12 ; training loss : 23135.637836200593\n",
      "for epoch 13 ; training loss : 21133.233621082967\n",
      "for epoch 14 ; training loss : 19414.246509732482\n",
      "for epoch 15 ; training loss : 17927.539911345943\n",
      "for epoch 16 ; training loss : 16640.403962912154\n",
      "for epoch 17 ; training loss : 15528.574944733005\n",
      "for epoch 18 ; training loss : 14571.431129552337\n",
      "for epoch 19 ; training loss : 13750.290548433175\n",
      "for epoch 20 ; training loss : 13047.66679432716\n",
      "for epoch 21 ; training loss : 12447.237251236253\n",
      "for epoch 22 ; training loss : 11934.06257865507\n",
      "for epoch 23 ; training loss : 11494.384041927555\n",
      "for epoch 24 ; training loss : 11116.042748260606\n",
      "for epoch 25 ; training loss : 10788.504200920725\n",
      "for epoch 26 ; training loss : 10502.73076262066\n",
      "for epoch 27 ; training loss : 10251.078767915773\n",
      "for epoch 28 ; training loss : 10027.372594552347\n",
      "for epoch 29 ; training loss : 9826.616431130096\n",
      "for epoch 30 ; training loss : 9644.791013913113\n",
      "for epoch 31 ; training loss : 9478.645007528165\n",
      "for epoch 32 ; training loss : 9325.615700528815\n",
      "for epoch 33 ; training loss : 9183.702483632966\n",
      "for epoch 34 ; training loss : 9051.243366745417\n",
      "for epoch 35 ; training loss : 8926.914482299515\n",
      "for epoch 36 ; training loss : 8809.662099745212\n",
      "for epoch 37 ; training loss : 8698.644000884233\n",
      "for epoch 38 ; training loss : 8593.178874199599\n",
      "for epoch 39 ; training loss : 8492.64337411552\n",
      "for epoch 40 ; training loss : 8396.596321179713\n",
      "for epoch 41 ; training loss : 8304.651274029398\n",
      "for epoch 42 ; training loss : 8216.470713854746\n",
      "for epoch 43 ; training loss : 8131.73697569035\n",
      "for epoch 44 ; training loss : 8050.200902716722\n",
      "for epoch 45 ; training loss : 7971.601211803529\n",
      "for epoch 46 ; training loss : 7895.764651600199\n",
      "for epoch 47 ; training loss : 7822.519485126744\n",
      "for epoch 48 ; training loss : 7751.759369058157\n",
      "for epoch 49 ; training loss : 7683.30751201816\n",
      "*** Working on model 5000 , iter 7 ***\n",
      "for epoch 0 ; training loss : 262853.58598279953\n",
      "for epoch 1 ; training loss : 204402.0440519997\n",
      "for epoch 2 ; training loss : 157990.81679936266\n",
      "for epoch 3 ; training loss : 121339.559148737\n",
      "for epoch 4 ; training loss : 92843.9740276747\n",
      "for epoch 5 ; training loss : 71246.7483474037\n",
      "for epoch 6 ; training loss : 55468.52515961137\n",
      "for epoch 7 ; training loss : 44407.21246208367\n",
      "for epoch 8 ; training loss : 36872.1232494014\n",
      "for epoch 9 ; training loss : 31715.38702522806\n",
      "for epoch 10 ; training loss : 28023.68654590845\n",
      "for epoch 11 ; training loss : 25198.230534741422\n",
      "for epoch 12 ; training loss : 22902.2390651045\n",
      "for epoch 13 ; training loss : 20962.573223682386\n",
      "for epoch 14 ; training loss : 19291.430486448\n",
      "for epoch 15 ; training loss : 17840.763123468118\n",
      "for epoch 16 ; training loss : 16580.372429658542\n",
      "for epoch 17 ; training loss : 15487.835727198108\n",
      "for epoch 18 ; training loss : 14543.897626132592\n",
      "for epoch 19 ; training loss : 13731.104366480722\n",
      "for epoch 20 ; training loss : 13033.01976308247\n",
      "for epoch 21 ; training loss : 12434.224055819737\n",
      "for epoch 22 ; training loss : 11920.584082623685\n",
      "for epoch 23 ; training loss : 11479.02755191789\n",
      "for epoch 24 ; training loss : 11097.900769717293\n",
      "for epoch 25 ; training loss : 10767.055061470164\n",
      "for epoch 26 ; training loss : 10477.692251002769\n",
      "for epoch 27 ; training loss : 10222.500248458717\n",
      "for epoch 28 ; training loss : 9995.455186781084\n",
      "for epoch 29 ; training loss : 9791.598237594502\n",
      "for epoch 30 ; training loss : 9607.007861605103\n",
      "for epoch 31 ; training loss : 9438.4752757143\n",
      "for epoch 32 ; training loss : 9283.384298406745\n",
      "for epoch 33 ; training loss : 9139.77320130196\n",
      "for epoch 34 ; training loss : 9006.01752568601\n",
      "for epoch 35 ; training loss : 8880.742236642312\n",
      "for epoch 36 ; training loss : 8762.88135766835\n",
      "for epoch 37 ; training loss : 8651.544100526226\n",
      "for epoch 38 ; training loss : 8545.975291090803\n",
      "for epoch 39 ; training loss : 8445.584426187968\n",
      "for epoch 40 ; training loss : 8349.844067931557\n",
      "for epoch 41 ; training loss : 8258.312259004044\n",
      "for epoch 42 ; training loss : 8170.667485957616\n",
      "for epoch 43 ; training loss : 8086.590608135124\n",
      "for epoch 44 ; training loss : 8005.834813616359\n",
      "for epoch 45 ; training loss : 7928.123181066752\n",
      "for epoch 46 ; training loss : 7853.238241287287\n",
      "for epoch 47 ; training loss : 7780.9317313204665\n",
      "for epoch 48 ; training loss : 7711.109295920385\n",
      "for epoch 49 ; training loss : 7643.630488074996\n",
      "*** Working on model 5000 , iter 8 ***\n",
      "for epoch 0 ; training loss : 253857.16247344017\n",
      "for epoch 1 ; training loss : 196353.75080682803\n",
      "for epoch 2 ; training loss : 151013.34303995315\n",
      "for epoch 3 ; training loss : 115445.5585281991\n",
      "for epoch 4 ; training loss : 87935.12451614894\n",
      "for epoch 5 ; training loss : 67160.62573714636\n",
      "for epoch 6 ; training loss : 52031.25316999946\n",
      "for epoch 7 ; training loss : 41462.474804383295\n",
      "for epoch 8 ; training loss : 34296.88312266792\n",
      "for epoch 9 ; training loss : 29422.233018495157\n",
      "for epoch 10 ; training loss : 25956.30684043254\n",
      "for epoch 11 ; training loss : 23323.43162900764\n",
      "for epoch 12 ; training loss : 21200.56346426808\n",
      "for epoch 13 ; training loss : 19421.653593989788\n",
      "for epoch 14 ; training loss : 17901.6481004908\n",
      "for epoch 15 ; training loss : 16593.36517978192\n",
      "for epoch 16 ; training loss : 15466.623216110573\n",
      "for epoch 17 ; training loss : 14498.350736402808\n",
      "for epoch 18 ; training loss : 13668.722809347008\n",
      "for epoch 19 ; training loss : 12959.938765737053\n",
      "for epoch 20 ; training loss : 12355.457456777982\n",
      "for epoch 21 ; training loss : 11839.915200549149\n",
      "for epoch 22 ; training loss : 11399.314810241543\n",
      "for epoch 23 ; training loss : 11021.30380890091\n",
      "for epoch 24 ; training loss : 10695.080242188742\n",
      "for epoch 25 ; training loss : 10411.46773892925\n",
      "for epoch 26 ; training loss : 10162.641775938595\n",
      "for epoch 27 ; training loss : 9942.18999007994\n",
      "for epoch 28 ; training loss : 9744.928267999174\n",
      "for epoch 29 ; training loss : 9566.787768389695\n",
      "for epoch 30 ; training loss : 9404.412556085637\n",
      "for epoch 31 ; training loss : 9255.165438966098\n",
      "for epoch 32 ; training loss : 9116.9692157713\n",
      "for epoch 33 ; training loss : 8988.155446420325\n",
      "for epoch 34 ; training loss : 8867.420546483641\n",
      "for epoch 35 ; training loss : 8753.726971539145\n",
      "for epoch 36 ; training loss : 8646.178829496235\n",
      "for epoch 37 ; training loss : 8544.098336646464\n",
      "for epoch 38 ; training loss : 8446.925171240091\n",
      "for epoch 39 ; training loss : 8354.122294658431\n",
      "for epoch 40 ; training loss : 8265.293451939058\n",
      "for epoch 41 ; training loss : 8180.100545778172\n",
      "for epoch 42 ; training loss : 8098.26569200118\n",
      "for epoch 43 ; training loss : 8019.5264534955495\n",
      "for epoch 44 ; training loss : 7943.676291611486\n",
      "for epoch 45 ; training loss : 7870.482553511683\n",
      "for epoch 46 ; training loss : 7799.7983992020745\n",
      "for epoch 47 ; training loss : 7731.483353596355\n",
      "for epoch 48 ; training loss : 7665.413711791509\n",
      "for epoch 49 ; training loss : 7601.466654649601\n",
      "*** Working on model 5000 , iter 9 ***\n",
      "for epoch 0 ; training loss : 255144.8036776632\n",
      "for epoch 1 ; training loss : 196779.83122815614\n",
      "for epoch 2 ; training loss : 151068.57040146738\n",
      "for epoch 3 ; training loss : 115452.71948146046\n",
      "for epoch 4 ; training loss : 88107.09001654666\n",
      "for epoch 5 ; training loss : 67623.0376542256\n",
      "for epoch 6 ; training loss : 52823.23197522579\n",
      "for epoch 7 ; training loss : 42539.91498535371\n",
      "for epoch 8 ; training loss : 35554.21449085469\n",
      "for epoch 9 ; training loss : 30740.9347054807\n",
      "for epoch 10 ; training loss : 27244.909206380806\n",
      "for epoch 11 ; training loss : 24527.84898145843\n",
      "for epoch 12 ; training loss : 22295.64152219193\n",
      "for epoch 13 ; training loss : 20399.096978431917\n",
      "for epoch 14 ; training loss : 18762.07339245905\n",
      "for epoch 15 ; training loss : 17342.077317103132\n",
      "for epoch 16 ; training loss : 16111.037272712918\n",
      "for epoch 17 ; training loss : 15047.226056157568\n",
      "for epoch 18 ; training loss : 14131.844842209044\n",
      "for epoch 19 ; training loss : 13347.236294056842\n",
      "for epoch 20 ; training loss : 12676.684770397085\n",
      "for epoch 21 ; training loss : 12104.605270363936\n",
      "for epoch 22 ; training loss : 11616.527696321165\n",
      "for epoch 23 ; training loss : 11199.064001071572\n",
      "for epoch 24 ; training loss : 10840.34770157194\n",
      "for epoch 25 ; training loss : 10530.079768375675\n",
      "for epoch 26 ; training loss : 10259.553523133247\n",
      "for epoch 27 ; training loss : 10021.401933816687\n",
      "for epoch 28 ; training loss : 9809.579081309785\n",
      "for epoch 29 ; training loss : 9619.368035604162\n",
      "for epoch 30 ; training loss : 9446.955106335168\n",
      "for epoch 31 ; training loss : 9289.268430017168\n",
      "for epoch 32 ; training loss : 9143.846666866499\n",
      "for epoch 33 ; training loss : 9008.797025580308\n",
      "for epoch 34 ; training loss : 8882.556120487869\n",
      "for epoch 35 ; training loss : 8763.960511913698\n",
      "for epoch 36 ; training loss : 8652.054630250397\n",
      "for epoch 37 ; training loss : 8546.036245373325\n",
      "for epoch 38 ; training loss : 8445.25008646294\n",
      "for epoch 39 ; training loss : 8349.189942639467\n",
      "for epoch 40 ; training loss : 8257.397557066288\n",
      "for epoch 41 ; training loss : 8169.526812846554\n",
      "for epoch 42 ; training loss : 8085.196086034935\n",
      "for epoch 43 ; training loss : 8004.125395847081\n",
      "for epoch 44 ; training loss : 7926.098515225171\n",
      "for epoch 45 ; training loss : 7850.948682171194\n",
      "for epoch 46 ; training loss : 7778.44903156301\n",
      "for epoch 47 ; training loss : 7708.428241880909\n",
      "for epoch 48 ; training loss : 7640.772957879453\n",
      "for epoch 49 ; training loss : 7575.303874122954\n",
      "*** Working on model 10000 , iter 0 ***\n",
      "for epoch 0 ; training loss : 226521.7129491642\n",
      "for epoch 1 ; training loss : 139868.00195453147\n",
      "for epoch 2 ; training loss : 88857.57474251481\n",
      "for epoch 3 ; training loss : 59252.73022159119\n",
      "for epoch 4 ; training loss : 42643.290226867306\n",
      "for epoch 5 ; training loss : 33356.53538088025\n",
      "for epoch 6 ; training loss : 27720.68377984586\n",
      "for epoch 7 ; training loss : 23804.41326063397\n",
      "for epoch 8 ; training loss : 20789.193212368355\n",
      "for epoch 9 ; training loss : 18361.23356396325\n",
      "for epoch 10 ; training loss : 16386.687156306463\n",
      "for epoch 11 ; training loss : 14788.160939562156\n",
      "for epoch 12 ; training loss : 13505.375876896782\n",
      "for epoch 13 ; training loss : 12484.316993397006\n",
      "for epoch 14 ; training loss : 11675.475013404157\n",
      "for epoch 15 ; training loss : 11034.51842720632\n",
      "for epoch 16 ; training loss : 10523.143077503446\n",
      "for epoch 17 ; training loss : 10109.781045180483\n",
      "for epoch 18 ; training loss : 9769.49049416995\n",
      "for epoch 19 ; training loss : 9483.426599245591\n",
      "for epoch 20 ; training loss : 9237.808698988476\n",
      "for epoch 21 ; training loss : 9022.72381033123\n",
      "for epoch 22 ; training loss : 8831.088638219313\n",
      "for epoch 23 ; training loss : 8657.899506523758\n",
      "for epoch 24 ; training loss : 8499.523738017306\n",
      "for epoch 25 ; training loss : 8353.368544567988\n",
      "for epoch 26 ; training loss : 8217.554180464547\n",
      "for epoch 27 ; training loss : 8090.603138984679\n",
      "for epoch 28 ; training loss : 7971.388529228992\n",
      "for epoch 29 ; training loss : 7858.990548173795\n",
      "for epoch 30 ; training loss : 7752.695684800827\n",
      "for epoch 31 ; training loss : 7651.883941804292\n",
      "for epoch 32 ; training loss : 7556.069205554028\n",
      "for epoch 33 ; training loss : 7464.785579772364\n",
      "for epoch 34 ; training loss : 7377.662555635819\n",
      "for epoch 35 ; training loss : 7294.364305140218\n",
      "for epoch 36 ; training loss : 7214.587521472946\n",
      "for epoch 37 ; training loss : 7138.0833066745545\n",
      "for epoch 38 ; training loss : 7064.585228255633\n",
      "for epoch 39 ; training loss : 6993.889284248555\n",
      "for epoch 40 ; training loss : 6925.811758523443\n",
      "for epoch 41 ; training loss : 6860.186827493395\n",
      "for epoch 42 ; training loss : 6796.81623363178\n",
      "for epoch 43 ; training loss : 6735.562922141515\n",
      "for epoch 44 ; training loss : 6676.285180681534\n",
      "for epoch 45 ; training loss : 6618.862899785702\n",
      "for epoch 46 ; training loss : 6563.215484856162\n",
      "for epoch 47 ; training loss : 6509.21755587247\n",
      "for epoch 48 ; training loss : 6456.77371216313\n",
      "for epoch 49 ; training loss : 6405.794211983757\n",
      "*** Working on model 10000 , iter 1 ***\n",
      "for epoch 0 ; training loss : 227795.34300195402\n",
      "for epoch 1 ; training loss : 140623.06306741468\n",
      "for epoch 2 ; training loss : 88847.19757473271\n",
      "for epoch 3 ; training loss : 58640.8819044407\n",
      "for epoch 4 ; training loss : 41703.8375280403\n",
      "for epoch 5 ; training loss : 32308.67155000678\n",
      "for epoch 6 ; training loss : 26683.422994006774\n",
      "for epoch 7 ; training loss : 22832.11416964322\n",
      "for epoch 8 ; training loss : 19908.61120737779\n",
      "for epoch 9 ; training loss : 17587.85188187717\n",
      "for epoch 10 ; training loss : 15728.93664300639\n",
      "for epoch 11 ; training loss : 14247.72188300098\n",
      "for epoch 12 ; training loss : 13077.638254323916\n",
      "for epoch 13 ; training loss : 12159.628195753576\n",
      "for epoch 14 ; training loss : 11440.542052247882\n",
      "for epoch 15 ; training loss : 10874.154716786714\n",
      "for epoch 16 ; training loss : 10422.287496725185\n",
      "for epoch 17 ; training loss : 10054.729349428919\n",
      "for epoch 18 ; training loss : 9748.724484115344\n",
      "for epoch 19 ; training loss : 9487.742047469605\n",
      "for epoch 20 ; training loss : 9260.101438442089\n",
      "for epoch 21 ; training loss : 9057.682862789989\n",
      "for epoch 22 ; training loss : 8874.84328681615\n",
      "for epoch 23 ; training loss : 8707.658287179474\n",
      "for epoch 24 ; training loss : 8553.306424619659\n",
      "for epoch 25 ; training loss : 8409.743567596252\n",
      "for epoch 26 ; training loss : 8275.49587626048\n",
      "for epoch 27 ; training loss : 8149.363642105833\n",
      "for epoch 28 ; training loss : 8030.412036375841\n",
      "for epoch 29 ; training loss : 7917.936188928783\n",
      "for epoch 30 ; training loss : 7811.277775474242\n",
      "for epoch 31 ; training loss : 7709.906776941149\n",
      "for epoch 32 ; training loss : 7613.359544765626\n",
      "for epoch 33 ; training loss : 7521.246419945674\n",
      "for epoch 34 ; training loss : 7433.2435819645325\n",
      "for epoch 35 ; training loss : 7349.000038412669\n",
      "for epoch 36 ; training loss : 7268.228827707077\n",
      "for epoch 37 ; training loss : 7190.688750741407\n",
      "for epoch 38 ; training loss : 7116.162519324913\n",
      "for epoch 39 ; training loss : 7044.407443721793\n",
      "for epoch 40 ; training loss : 6975.267781177219\n",
      "for epoch 41 ; training loss : 6908.572144201797\n",
      "for epoch 42 ; training loss : 6844.171238376992\n",
      "for epoch 43 ; training loss : 6781.903228634052\n",
      "for epoch 44 ; training loss : 6721.658442042564\n",
      "for epoch 45 ; training loss : 6663.290224889541\n",
      "for epoch 46 ; training loss : 6606.719160450964\n",
      "for epoch 47 ; training loss : 6551.82650163335\n",
      "for epoch 48 ; training loss : 6498.521040955533\n",
      "for epoch 49 ; training loss : 6446.714187750276\n",
      "*** Working on model 10000 , iter 2 ***\n",
      "for epoch 0 ; training loss : 231389.71750006132\n",
      "for epoch 1 ; training loss : 143629.94888649508\n",
      "for epoch 2 ; training loss : 91351.5069819165\n",
      "for epoch 3 ; training loss : 60446.04849929108\n",
      "for epoch 4 ; training loss : 42823.19892876173\n",
      "for epoch 5 ; training loss : 32944.24580424675\n",
      "for epoch 6 ; training loss : 27046.943158267066\n",
      "for epoch 7 ; training loss : 23045.363870891466\n",
      "for epoch 8 ; training loss : 20022.83606682252\n",
      "for epoch 9 ; training loss : 17624.037194598583\n",
      "for epoch 10 ; training loss : 15699.615470834542\n",
      "for epoch 11 ; training loss : 14164.231315620364\n",
      "for epoch 12 ; training loss : 12951.343167115701\n",
      "for epoch 13 ; training loss : 12001.384094860288\n",
      "for epoch 14 ; training loss : 11260.082700357365\n",
      "for epoch 15 ; training loss : 10679.704412425974\n",
      "for epoch 16 ; training loss : 10220.11967888911\n",
      "for epoch 17 ; training loss : 9849.501159020845\n",
      "for epoch 18 ; training loss : 9543.568812062564\n",
      "for epoch 19 ; training loss : 9284.705233450975\n",
      "for epoch 20 ; training loss : 9060.380497891812\n",
      "for epoch 21 ; training loss : 8861.95937444827\n",
      "for epoch 22 ; training loss : 8683.433321194523\n",
      "for epoch 23 ; training loss : 8520.63433077099\n",
      "for epoch 24 ; training loss : 8370.675899041264\n",
      "for epoch 25 ; training loss : 8231.486740949927\n",
      "for epoch 26 ; training loss : 8101.4914689844045\n",
      "for epoch 27 ; training loss : 7979.47896893775\n",
      "for epoch 28 ; training loss : 7864.51802417374\n",
      "for epoch 29 ; training loss : 7755.892855923215\n",
      "for epoch 30 ; training loss : 7652.922552554737\n",
      "for epoch 31 ; training loss : 7555.155832660708\n",
      "for epoch 32 ; training loss : 7462.1114817557755\n",
      "for epoch 33 ; training loss : 7373.4257135405205\n",
      "for epoch 34 ; training loss : 7288.75136876712\n",
      "for epoch 35 ; training loss : 7207.7812234354205\n",
      "for epoch 36 ; training loss : 7130.236401802569\n",
      "for epoch 37 ; training loss : 7055.853959688931\n",
      "for epoch 38 ; training loss : 6984.414029416352\n",
      "for epoch 39 ; training loss : 6915.741288730074\n",
      "for epoch 40 ; training loss : 6849.613383086165\n",
      "for epoch 41 ; training loss : 6785.854211642931\n",
      "for epoch 42 ; training loss : 6724.340481696476\n",
      "for epoch 43 ; training loss : 6664.908688263331\n",
      "for epoch 44 ; training loss : 6607.417644611662\n",
      "for epoch 45 ; training loss : 6551.78187446055\n",
      "for epoch 46 ; training loss : 6497.874094050509\n",
      "for epoch 47 ; training loss : 6445.586808087668\n",
      "for epoch 48 ; training loss : 6394.826418124838\n",
      "for epoch 49 ; training loss : 6345.497955309285\n",
      "*** Working on model 10000 , iter 3 ***\n",
      "for epoch 0 ; training loss : 231067.2808949342\n",
      "for epoch 1 ; training loss : 143376.144659339\n",
      "for epoch 2 ; training loss : 91402.00849695154\n",
      "for epoch 3 ; training loss : 60767.280757491506\n",
      "for epoch 4 ; training loss : 43308.66140297086\n",
      "for epoch 5 ; training loss : 33520.14464573539\n",
      "for epoch 6 ; training loss : 27672.56784567004\n",
      "for epoch 7 ; training loss : 23693.510816926893\n",
      "for epoch 8 ; training loss : 20667.56441873651\n",
      "for epoch 9 ; training loss : 18240.083726166995\n",
      "for epoch 10 ; training loss : 16265.70981832077\n",
      "for epoch 11 ; training loss : 14665.530545423913\n",
      "for epoch 12 ; training loss : 13380.415368576476\n",
      "for epoch 13 ; training loss : 12357.724860564711\n",
      "for epoch 14 ; training loss : 11548.664744147449\n",
      "for epoch 15 ; training loss : 10909.05596251128\n",
      "for epoch 16 ; training loss : 10400.499984501439\n",
      "for epoch 17 ; training loss : 9991.078376305628\n",
      "for epoch 18 ; training loss : 9655.469427041957\n",
      "for epoch 19 ; training loss : 9374.420554457265\n",
      "for epoch 20 ; training loss : 9133.775686100824\n",
      "for epoch 21 ; training loss : 8923.42103583843\n",
      "for epoch 22 ; training loss : 8736.218148383603\n",
      "for epoch 23 ; training loss : 8567.13126180027\n",
      "for epoch 24 ; training loss : 8412.575623401382\n",
      "for epoch 25 ; training loss : 8269.949149367378\n",
      "for epoch 26 ; training loss : 8137.366825602094\n",
      "for epoch 27 ; training loss : 8013.424473510619\n",
      "for epoch 28 ; training loss : 7897.018675832922\n",
      "for epoch 29 ; training loss : 7787.243270701452\n",
      "for epoch 30 ; training loss : 7683.456752154918\n",
      "for epoch 31 ; training loss : 7585.042573356186\n",
      "for epoch 32 ; training loss : 7491.478680288492\n",
      "for epoch 33 ; training loss : 7402.346405456425\n",
      "for epoch 34 ; training loss : 7317.295076196927\n",
      "for epoch 35 ; training loss : 7235.972955380683\n",
      "for epoch 36 ; training loss : 7158.084705262852\n",
      "for epoch 37 ; training loss : 7083.373365577241\n",
      "for epoch 38 ; training loss : 7011.612373596483\n",
      "for epoch 39 ; training loss : 6942.57809010975\n",
      "for epoch 40 ; training loss : 6876.071576106915\n",
      "for epoch 41 ; training loss : 6811.950145146839\n",
      "for epoch 42 ; training loss : 6750.042933567369\n",
      "for epoch 43 ; training loss : 6690.223510909549\n",
      "for epoch 44 ; training loss : 6632.391764474058\n",
      "for epoch 45 ; training loss : 6576.371709450439\n",
      "for epoch 46 ; training loss : 6522.079069702406\n",
      "for epoch 47 ; training loss : 6469.431470256677\n",
      "for epoch 48 ; training loss : 6418.31934550597\n",
      "for epoch 49 ; training loss : 6368.652614673298\n",
      "*** Working on model 10000 , iter 4 ***\n",
      "for epoch 0 ; training loss : 226150.0188950524\n",
      "for epoch 1 ; training loss : 139158.85421149718\n",
      "for epoch 2 ; training loss : 87564.71034142002\n",
      "for epoch 3 ; training loss : 57477.8824359196\n",
      "for epoch 4 ; training loss : 40654.94385301834\n",
      "for epoch 5 ; training loss : 31406.91267979436\n",
      "for epoch 6 ; training loss : 25944.84061415959\n",
      "for epoch 7 ; training loss : 22242.64656645886\n",
      "for epoch 8 ; training loss : 19443.013547863055\n",
      "for epoch 9 ; training loss : 17221.577496495418\n",
      "for epoch 10 ; training loss : 15440.547308349865\n",
      "for epoch 11 ; training loss : 14018.746290479117\n",
      "for epoch 12 ; training loss : 12892.152041123889\n",
      "for epoch 13 ; training loss : 12004.155987292615\n",
      "for epoch 14 ; training loss : 11304.340465672984\n",
      "for epoch 15 ; training loss : 10749.210793394479\n",
      "for epoch 16 ; training loss : 10302.96527879703\n",
      "for epoch 17 ; training loss : 9937.446208129739\n",
      "for epoch 18 ; training loss : 9631.527913042726\n",
      "for epoch 19 ; training loss : 9369.768791739607\n",
      "for epoch 20 ; training loss : 9141.124029574436\n",
      "for epoch 21 ; training loss : 8937.87029466535\n",
      "for epoch 22 ; training loss : 8754.517212734558\n",
      "for epoch 23 ; training loss : 8587.180590907934\n",
      "for epoch 24 ; training loss : 8433.0468419127\n",
      "for epoch 25 ; training loss : 8290.055531994672\n",
      "for epoch 26 ; training loss : 8156.633969233939\n",
      "for epoch 27 ; training loss : 8031.574788561473\n",
      "for epoch 28 ; training loss : 7913.891852851564\n",
      "for epoch 29 ; training loss : 7802.802849668253\n",
      "for epoch 30 ; training loss : 7697.653754635161\n",
      "for epoch 31 ; training loss : 7597.8887881375485\n",
      "for epoch 32 ; training loss : 7503.031865905101\n",
      "for epoch 33 ; training loss : 7412.658122559169\n",
      "for epoch 34 ; training loss : 7326.410434675866\n",
      "for epoch 35 ; training loss : 7243.968459629516\n",
      "for epoch 36 ; training loss : 7165.039815901051\n",
      "for epoch 37 ; training loss : 7089.34734601574\n",
      "for epoch 38 ; training loss : 7016.640612293915\n",
      "for epoch 39 ; training loss : 6946.692488560006\n",
      "for epoch 40 ; training loss : 6879.342510977498\n",
      "for epoch 41 ; training loss : 6814.393292233668\n",
      "for epoch 42 ; training loss : 6751.72480291914\n",
      "for epoch 43 ; training loss : 6691.183789453236\n",
      "for epoch 44 ; training loss : 6632.633645519534\n",
      "for epoch 45 ; training loss : 6575.932883734786\n",
      "for epoch 46 ; training loss : 6520.981096130257\n",
      "for epoch 47 ; training loss : 6467.6835437139125\n",
      "for epoch 48 ; training loss : 6415.929945758664\n",
      "for epoch 49 ; training loss : 6365.6359323731085\n",
      "*** Working on model 10000 , iter 5 ***\n",
      "for epoch 0 ; training loss : 229085.32897409785\n",
      "for epoch 1 ; training loss : 141947.49946235237\n",
      "for epoch 2 ; training loss : 90160.94174823369\n",
      "for epoch 3 ; training loss : 59736.00269327627\n",
      "for epoch 4 ; training loss : 42487.60637757616\n",
      "for epoch 5 ; training loss : 32864.78977158059\n",
      "for epoch 6 ; training loss : 27137.418362770724\n",
      "for epoch 7 ; training loss : 23253.824393160936\n",
      "for epoch 8 ; training loss : 20315.32900457183\n",
      "for epoch 9 ; training loss : 17974.249033502947\n",
      "for epoch 10 ; training loss : 16084.737954663287\n",
      "for epoch 11 ; training loss : 14564.696901737901\n",
      "for epoch 12 ; training loss : 13351.200767844093\n",
      "for epoch 13 ; training loss : 12388.806203509834\n",
      "for epoch 14 ; training loss : 11627.480663911323\n",
      "for epoch 15 ; training loss : 11023.075339666451\n",
      "for epoch 16 ; training loss : 10538.434542365489\n",
      "for epoch 17 ; training loss : 10143.510010508358\n",
      "for epoch 18 ; training loss : 9815.160622208306\n",
      "for epoch 19 ; training loss : 9536.198732219578\n",
      "for epoch 20 ; training loss : 9294.175405887901\n",
      "for epoch 21 ; training loss : 9080.24717331534\n",
      "for epoch 22 ; training loss : 8888.18693311591\n",
      "for epoch 23 ; training loss : 8713.540267056218\n",
      "for epoch 24 ; training loss : 8553.148513318112\n",
      "for epoch 25 ; training loss : 8404.675717289301\n",
      "for epoch 26 ; training loss : 8266.392999890173\n",
      "for epoch 27 ; training loss : 8136.9354494486215\n",
      "for epoch 28 ; training loss : 8015.264658512737\n",
      "for epoch 29 ; training loss : 7900.574766727517\n",
      "for epoch 30 ; training loss : 7792.150344802303\n",
      "for epoch 31 ; training loss : 7689.3984919040595\n",
      "for epoch 32 ; training loss : 7591.80234321213\n",
      "for epoch 33 ; training loss : 7498.910329832928\n",
      "for epoch 34 ; training loss : 7410.350639114273\n",
      "for epoch 35 ; training loss : 7325.789978707911\n",
      "for epoch 36 ; training loss : 7244.886579776881\n",
      "for epoch 37 ; training loss : 7167.388465650147\n",
      "for epoch 38 ; training loss : 7093.030946137384\n",
      "for epoch 39 ; training loss : 7021.6159753697575\n",
      "for epoch 40 ; training loss : 6952.920071291417\n",
      "for epoch 41 ; training loss : 6886.773332234883\n",
      "for epoch 42 ; training loss : 6822.9819443660235\n",
      "for epoch 43 ; training loss : 6761.424735496592\n",
      "for epoch 44 ; training loss : 6701.946561074223\n",
      "for epoch 45 ; training loss : 6644.404554143781\n",
      "for epoch 46 ; training loss : 6588.6553406979365\n",
      "for epoch 47 ; training loss : 6534.620427176138\n",
      "for epoch 48 ; training loss : 6482.163674631578\n",
      "for epoch 49 ; training loss : 6431.235853259135\n",
      "*** Working on model 10000 , iter 6 ***\n",
      "for epoch 0 ; training loss : 228566.20692104136\n",
      "for epoch 1 ; training loss : 141872.69679160404\n",
      "for epoch 2 ; training loss : 90430.52612095608\n",
      "for epoch 3 ; training loss : 60042.55704824999\n",
      "for epoch 4 ; training loss : 42684.60508003947\n",
      "for epoch 5 ; training loss : 32927.10694718349\n",
      "for epoch 6 ; training loss : 27089.307524762466\n",
      "for epoch 7 ; training loss : 23125.993901781843\n",
      "for epoch 8 ; training loss : 20132.64720634464\n",
      "for epoch 9 ; training loss : 17755.038979585093\n",
      "for epoch 10 ; training loss : 15843.345496180296\n",
      "for epoch 11 ; training loss : 14312.6270669761\n",
      "for epoch 12 ; training loss : 13097.534526726922\n",
      "for epoch 13 ; training loss : 12140.27667559596\n",
      "for epoch 14 ; training loss : 11388.58184321909\n",
      "for epoch 15 ; training loss : 10796.359349832077\n",
      "for epoch 16 ; training loss : 10324.896976210337\n",
      "for epoch 17 ; training loss : 9943.140779201527\n",
      "for epoch 18 ; training loss : 9627.291218079234\n",
      "for epoch 19 ; training loss : 9359.878625641475\n",
      "for epoch 20 ; training loss : 9128.378038124592\n",
      "for epoch 21 ; training loss : 8924.016438029223\n",
      "for epoch 22 ; training loss : 8740.649312848953\n",
      "for epoch 23 ; training loss : 8573.960649361848\n",
      "for epoch 24 ; training loss : 8420.909309024173\n",
      "for epoch 25 ; training loss : 8279.213760617698\n",
      "for epoch 26 ; training loss : 8147.1624268102605\n",
      "for epoch 27 ; training loss : 8023.491829625738\n",
      "for epoch 28 ; training loss : 7907.192004900797\n",
      "for epoch 29 ; training loss : 7797.442124720561\n",
      "for epoch 30 ; training loss : 7693.628674729451\n",
      "for epoch 31 ; training loss : 7595.171245431993\n",
      "for epoch 32 ; training loss : 7501.581800620654\n",
      "for epoch 33 ; training loss : 7412.452729323442\n",
      "for epoch 34 ; training loss : 7327.374161832908\n",
      "for epoch 35 ; training loss : 7246.048682557932\n",
      "for epoch 36 ; training loss : 7168.192565528334\n",
      "for epoch 37 ; training loss : 7093.576839791916\n",
      "for epoch 38 ; training loss : 7021.930595568327\n",
      "for epoch 39 ; training loss : 6953.03413532579\n",
      "for epoch 40 ; training loss : 6886.71416759424\n",
      "for epoch 41 ; training loss : 6822.802274329137\n",
      "for epoch 42 ; training loss : 6761.147451937533\n",
      "for epoch 43 ; training loss : 6701.583044063242\n",
      "for epoch 44 ; training loss : 6644.004961084545\n",
      "for epoch 45 ; training loss : 6588.227000130937\n",
      "for epoch 46 ; training loss : 6534.177044752956\n",
      "for epoch 47 ; training loss : 6481.750498396188\n",
      "for epoch 48 ; training loss : 6430.838753774897\n",
      "for epoch 49 ; training loss : 6381.388193777388\n",
      "*** Working on model 10000 , iter 7 ***\n",
      "for epoch 0 ; training loss : 230093.22461042926\n",
      "for epoch 1 ; training loss : 143271.71219749365\n",
      "for epoch 2 ; training loss : 91320.66525578144\n",
      "for epoch 3 ; training loss : 60635.57684118772\n",
      "for epoch 4 ; training loss : 43190.1496226117\n",
      "for epoch 5 ; training loss : 33414.99169525925\n",
      "for epoch 6 ; training loss : 27544.55076974661\n",
      "for epoch 7 ; training loss : 23523.998039264057\n",
      "for epoch 8 ; training loss : 20463.13066778006\n",
      "for epoch 9 ; training loss : 18019.53779386275\n",
      "for epoch 10 ; training loss : 16048.647754158636\n",
      "for epoch 11 ; training loss : 14467.473238255283\n",
      "for epoch 12 ; training loss : 13211.0741656615\n",
      "for epoch 13 ; training loss : 12221.24752396204\n",
      "for epoch 14 ; training loss : 11444.744400537573\n",
      "for epoch 15 ; training loss : 10834.37701452199\n",
      "for epoch 16 ; training loss : 10350.02569456246\n",
      "for epoch 17 ; training loss : 9959.363319870201\n",
      "for epoch 18 ; training loss : 9637.476919286506\n",
      "for epoch 19 ; training loss : 9365.952001284153\n",
      "for epoch 20 ; training loss : 9131.657390560737\n",
      "for epoch 21 ; training loss : 8925.309560748145\n",
      "for epoch 22 ; training loss : 8740.48908382938\n",
      "for epoch 23 ; training loss : 8572.721206513466\n",
      "for epoch 24 ; training loss : 8418.792196996626\n",
      "for epoch 25 ; training loss : 8276.353535441463\n",
      "for epoch 26 ; training loss : 8143.683683133975\n",
      "for epoch 27 ; training loss : 8019.524841971477\n",
      "for epoch 28 ; training loss : 7902.768090872982\n",
      "for epoch 29 ; training loss : 7792.6069789051435\n",
      "for epoch 30 ; training loss : 7688.340311212465\n",
      "for epoch 31 ; training loss : 7589.443404615162\n",
      "for epoch 32 ; training loss : 7495.405660172328\n",
      "for epoch 33 ; training loss : 7405.829206161681\n",
      "for epoch 34 ; training loss : 7320.3262252002605\n",
      "for epoch 35 ; training loss : 7238.596965349454\n",
      "for epoch 36 ; training loss : 7160.36117813384\n",
      "for epoch 37 ; training loss : 7085.344732266793\n",
      "for epoch 38 ; training loss : 7013.3118142448075\n",
      "for epoch 39 ; training loss : 6944.047975379177\n",
      "for epoch 40 ; training loss : 6877.349141940322\n",
      "for epoch 41 ; training loss : 6813.069687346811\n",
      "for epoch 42 ; training loss : 6751.030016927776\n",
      "for epoch 43 ; training loss : 6691.077736133477\n",
      "for epoch 44 ; training loss : 6633.077013878923\n",
      "for epoch 45 ; training loss : 6576.936676795245\n",
      "for epoch 46 ; training loss : 6522.563389501214\n",
      "for epoch 47 ; training loss : 6469.831612559254\n",
      "for epoch 48 ; training loss : 6418.674397638519\n",
      "for epoch 49 ; training loss : 6368.971801056865\n",
      "*** Working on model 10000 , iter 8 ***\n",
      "for epoch 0 ; training loss : 227616.19138376386\n",
      "for epoch 1 ; training loss : 141302.73008933268\n",
      "for epoch 2 ; training loss : 90080.49362489689\n",
      "for epoch 3 ; training loss : 59981.946034775174\n",
      "for epoch 4 ; training loss : 42898.87529680504\n",
      "for epoch 5 ; training loss : 33281.56851580134\n",
      "for epoch 6 ; training loss : 27446.820294130186\n",
      "for epoch 7 ; training loss : 23419.154345727584\n",
      "for epoch 8 ; training loss : 20349.550468293717\n",
      "for epoch 9 ; training loss : 17906.9994503513\n",
      "for epoch 10 ; training loss : 15946.612547071825\n",
      "for epoch 11 ; training loss : 14382.118822812074\n",
      "for epoch 12 ; training loss : 13145.464550134915\n",
      "for epoch 13 ; training loss : 12175.835037274697\n",
      "for epoch 14 ; training loss : 11418.126984120496\n",
      "for epoch 15 ; training loss : 10823.866286918466\n",
      "for epoch 16 ; training loss : 10352.630078109185\n",
      "for epoch 17 ; training loss : 9972.2005133339\n",
      "for epoch 18 ; training loss : 9658.070821250672\n",
      "for epoch 19 ; training loss : 9392.357408643336\n",
      "for epoch 20 ; training loss : 9162.362076053949\n",
      "for epoch 21 ; training loss : 8959.190190882699\n",
      "for epoch 22 ; training loss : 8776.646280300556\n",
      "for epoch 23 ; training loss : 8610.40440019175\n",
      "for epoch 24 ; training loss : 8457.433143911247\n",
      "for epoch 25 ; training loss : 8315.520005737402\n",
      "for epoch 26 ; training loss : 8183.059221810603\n",
      "for epoch 27 ; training loss : 8058.793989770027\n",
      "for epoch 28 ; training loss : 7941.716021007174\n",
      "for epoch 29 ; training loss : 7831.055710022381\n",
      "for epoch 30 ; training loss : 7726.19943934468\n",
      "for epoch 31 ; training loss : 7626.57761827833\n",
      "for epoch 32 ; training loss : 7531.719144575691\n",
      "for epoch 33 ; training loss : 7441.207109842144\n",
      "for epoch 34 ; training loss : 7354.732372665549\n",
      "for epoch 35 ; training loss : 7271.993440996932\n",
      "for epoch 36 ; training loss : 7192.721170931953\n",
      "for epoch 37 ; training loss : 7116.648642260479\n",
      "for epoch 38 ; training loss : 7043.532219500663\n",
      "for epoch 39 ; training loss : 6973.172541265318\n",
      "for epoch 40 ; training loss : 6905.394917449859\n",
      "for epoch 41 ; training loss : 6840.0377487798105\n",
      "for epoch 42 ; training loss : 6776.964603594155\n",
      "for epoch 43 ; training loss : 6715.998717911032\n",
      "for epoch 44 ; training loss : 6657.02531642784\n",
      "for epoch 45 ; training loss : 6599.8982058285765\n",
      "for epoch 46 ; training loss : 6544.535140357129\n",
      "for epoch 47 ; training loss : 6490.850581684012\n",
      "for epoch 48 ; training loss : 6438.75310491315\n",
      "for epoch 49 ; training loss : 6388.114947065282\n",
      "*** Working on model 10000 , iter 9 ***\n",
      "for epoch 0 ; training loss : 227466.62316470593\n",
      "for epoch 1 ; training loss : 141453.9699469027\n",
      "for epoch 2 ; training loss : 90182.26357570656\n",
      "for epoch 3 ; training loss : 59796.09872087068\n",
      "for epoch 4 ; training loss : 42427.130447871285\n",
      "for epoch 5 ; training loss : 32667.28945768258\n",
      "for epoch 6 ; training loss : 26826.095436753938\n",
      "for epoch 7 ; training loss : 22859.95382620598\n",
      "for epoch 8 ; training loss : 19869.094368722457\n",
      "for epoch 9 ; training loss : 17502.149113127904\n",
      "for epoch 10 ; training loss : 15608.83892909957\n",
      "for epoch 11 ; training loss : 14102.19938368877\n",
      "for epoch 12 ; training loss : 12914.444215245083\n",
      "for epoch 13 ; training loss : 11985.466309815005\n",
      "for epoch 14 ; training loss : 11260.950255912241\n",
      "for epoch 15 ; training loss : 10693.357422421934\n",
      "for epoch 16 ; training loss : 10243.140959814162\n",
      "for epoch 17 ; training loss : 9878.959829368818\n",
      "for epoch 18 ; training loss : 9577.218732196183\n",
      "for epoch 19 ; training loss : 9320.831263669126\n",
      "for epoch 20 ; training loss : 9097.7964785246\n",
      "for epoch 21 ; training loss : 8899.792757531148\n",
      "for epoch 22 ; training loss : 8721.137982413991\n",
      "for epoch 23 ; training loss : 8557.901967455997\n",
      "for epoch 24 ; training loss : 8407.309191651715\n",
      "for epoch 25 ; training loss : 8267.31299581076\n",
      "for epoch 26 ; training loss : 8136.509986800738\n",
      "for epoch 27 ; training loss : 8013.758745532963\n",
      "for epoch 28 ; training loss : 7898.112039190804\n",
      "for epoch 29 ; training loss : 7788.837553613412\n",
      "for epoch 30 ; training loss : 7685.378156329796\n",
      "for epoch 31 ; training loss : 7587.219747590079\n",
      "for epoch 32 ; training loss : 7493.890039217775\n",
      "for epoch 33 ; training loss : 7404.954704822889\n",
      "for epoch 34 ; training loss : 7320.052278656047\n",
      "for epoch 35 ; training loss : 7238.903487859003\n",
      "for epoch 36 ; training loss : 7161.207768735258\n",
      "for epoch 37 ; training loss : 7086.725708824\n",
      "for epoch 38 ; training loss : 7015.258900058689\n",
      "for epoch 39 ; training loss : 6946.57692922201\n",
      "for epoch 40 ; training loss : 6880.490045075188\n",
      "for epoch 41 ; training loss : 6816.82566077204\n",
      "for epoch 42 ; training loss : 6755.409264777845\n",
      "for epoch 43 ; training loss : 6696.107149530933\n",
      "for epoch 44 ; training loss : 6638.806021712098\n",
      "for epoch 45 ; training loss : 6583.325673187268\n",
      "for epoch 46 ; training loss : 6529.587726641989\n",
      "for epoch 47 ; training loss : 6477.488919515454\n",
      "for epoch 48 ; training loss : 6426.912516020995\n",
      "for epoch 49 ; training loss : 6377.811352756638\n"
     ]
    }
   ],
   "source": [
    "for dim in results_dict.keys():\n",
    "    for iter in range(10):\n",
    "        print(f'*** Working on model {dim} , iter {iter} ***')\n",
    "\n",
    "        model = Network(dim)\n",
    "        parameters = {k:v.detach() for k, v in model.named_parameters()}\n",
    "\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(model, params, (x.unsqueeze(0), )).squeeze(0)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "        ntk_init = get_ntk(fnet_single, parameters, x_ntk)\n",
    "\n",
    "        for epoch in range(50):\n",
    "            epoch_loss = 0\n",
    "            for x, y in dataset:\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss+=loss.item()\n",
    "\n",
    "            print(f\"for epoch {epoch} ; training loss : {epoch_loss}\")\n",
    "\n",
    "            parameters = {k:v.detach() for k, v in model.named_parameters()}\n",
    "            ntk = get_ntk(fnet_single, parameters, x_ntk)\n",
    "            rel_norm = get_relative_norm(ntk, ntk_init)\n",
    "            results_dict[dim][iter].append(rel_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a3abd550>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dim in results_dict.keys():\n",
    "    aa=np.mean(results_dict[dim], axis=0)\n",
    "    plt.plot(aa, label=dim)\n",
    "plt.xlabel('Itération')\n",
    "plt.ylabel(r'$\\frac{\\|\\theta_t - \\theta_0\\|^2}{\\|\\theta_0\\|^2}$')\n",
    "plt.title(\"Evolution de la norme relative du NTK au cours de l'entraînement\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
